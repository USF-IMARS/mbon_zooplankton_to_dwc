---
title: "Merge Metadata, Ocurrence and AphiaID"
author: "Sebastian DiGeronimo"
date: "2023-03-07"
output: html_document
---

TODO: check how to save UTF-8 encoding on csv files 
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
librarian::shelf(
    librarian, ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr,
    forcats, lubridate, glue, fs, magrittr, here,
    # broom # optional
    
    # additional
    openxlsx
)

library("conflicted")

conflicts_prefer(dplyr::filter,
                 dplyr::select)

# library("readxl")
# library("hablar") # might be useful when needing to fix names
# library("worrms")
# library("geosphere")


# obistools::event_fields()
# source(here::here("scripts","log_file_changes.R"))
# startup("log_edits")
# log_add()
```

```{r other-vars}
other_vars <- list()

# ---- recorded by info
other_vars$identifier       <- "Natalia Lopez Figueroa"
other_vars$identifier_orcid <- "https://orcid.org/0000-0002-7527-0481"

# ---- max depth per site
other_vars$max_depth <-
  tribble(
    ~site, ~maximumDepthInMeters,
     "MR",                  36.5,
     "LK",                  40.5,
     "WS",                    23,
     "54",                     3,
     "57",                     5,
     "9B",                    31
  )

# ---- set variables for calculation
other_vars$inpeller <- 26873 / 999999 # starting oct 2022 for 200/500 um
# net_area <- pi * 0.5^2

# Note: jamie dillution / pippete_vol_m_l is the same as nat dillution_factor
# 
# total num individual (sum all aliquots)  * vol sample water (known vol) / counted aliquot * split size / folson (mL per aliquot)
# 
# 
# mean = total num / counted aliquots 
# dilution factor = vol sample water / folson 
# 
which_one <- "nat"
other_vars$equation_zoo <- switch(
    which_one,
    # pretty it's `nat`
    "nat"    = expression(dillution_factor * mean * total_split_amount),
    
    
    "nat_old"    = expression(dillution_factor * mean * splits_analyzed),
    "nat2"   = expression(dillution_factor * mean * (splits_analyzed * split_size)^-1),
    "nat3"   = expression(dillution_factor * mean * (splits_analyzed)^-1),
    "jamie"  = expression((dillution / pipette_vol_m_l) * mean * splits_analyzed ),
    "jamie2" = expression(dillution * mean)
)
rm(which_one)
```


# Load functions if not already
This will load the functions needed if not already contain in the search path.
```{r load-functions}
source(here("scripts", "attach_funcs.R"))
func_attach()
rm(func_attach)
```

# Load data for OBIS conversion
```{r load-data}
aphia_id <- 
    here("data", "metadata") %>%
    dir_ls(regexp = "aphia_additional")

if (is_empty(aphia_id) | FALSE) {
  cli_alert_info("Creating a additional aphia ID file.")

  # ---- load aphia id
  aphia_id <-
    dir_ls(
      path = here("data", "metadata", "aphia_id"),
      regexp = "^[^~]*(aphia)+.*\\.csv$"
    ) %>%
    last_mod(.) %>%
    read_csv(show_col_types = FALSE) %>%
    mutate(
      aphiaID = str_extract(scientificNameID, "\\d+"),
      aphiaID = as.numeric(aphiaID),
      info = map(
        aphiaID,
        ~ tryCatch(
          {
            worrms::wm_record(.x)
          },
          error = function(e) {
            return(NULL)
          }
        )
      )
    ) %>%
    unnest(info, names_repair = janitor::make_clean_names) %>%
    select(!contains("_2"), -c(2:6))


  meta_file <-
    (file_expr(
      loc = here("data", "metadata"),
      file_base = "aphia_additional"
    ))[[2]] %>%
    eval()

  cli_alert_info("File name: {.file {basename(meta_file)}}")

  write_csv(aphia_id,
    file = meta_file,
    na   = ""
  )
} else {
  aphia_id <-
    aphia_id %>%
    last_mod()

  cli_alert_info("Loading file: {.file {basename(aphia_id)}}")

  aphia_id <- 
      read_csv(
          aphia_id,
          show_col_types = FALSE
  )
}
```

```{r load-data}
# ---- load metadata
meta_df <-
    dir_ls(
        path   = here("data", "metadata", "cruise_logsheets"),
        regexp = "^[^~]*(meta_)+.*\\.csv$") %>%
    last_mod(.) %>%
    read_csv(show_col_types = FALSE) %>%
    
    # this is when Natalia LF started analysis
    filter(date >= date("2017-06-18")) %>%
    
    mutate(locationID      = case_when(
                str_detect(station, "Mol")  ~ "MR",
                str_detect(station, "Loo")  ~ "LK",
                str_detect(station, "West") ~ "WS",
                str_detect(station, "9B")   ~ "9B",
                .default = station), 
           .after = station) %>%
    mutate(
        # Note: flowmeter is MF315
        net_size            = 0.5,
        net_area            = pi * 0.5^2,
        distance_m_2        = (flowmeter_out - flowmeter_in) * inpeller_constant,
        tow_speed_m_sec     = distance_m_2 / (tow_time_min * 60),
        volume_filt_cubic_m = net_area/4 * distance_m_2,
        split_size          = map_dbl(split_size, function(x) {
                                    out <- tryCatch({
                                    eval(parse(text = x))}, 
                                    error = function(e) {NA_integer_})}) 
       ) 

# ---- load zooplankton data
taxa_files <-
  dir_ls(
    path = here("data", "processed"),
    regexp = "all_merged_processed"
  ) %>%
  last_mod(.) %>%
  map_dfr(~ read_csv(., show_col_types = FALSE)) %>%
  mutate(site = str_replace(site, "MK", "MR")) %>%
  select(-mean_ind_dil_factor) %>%
  rowwise(aliquot_1:aliquot_3) %>%
  mutate(
    individualCount = 
        sum(aliquot_1,
            aliquot_2,
            aliquot_3,
            na.rm = TRUE
            ),
    .before = aliquot_1
  ) %>%
  ungroup()

# ---- merge metadata and taxa data
taxa_original <-
  taxa_files %>%
  left_join(
    meta_df,
    by = c(
      "cruise_id",
      "site" = "locationID",
      "mesh" = "mesh_size_um"
    )
  ) %>%
  
  # add max depth
  left_join(other_vars$max_depth) %>%

  mutate(
    # add min depth and uncertainty
    minimumDepthInMeters          = 0,
    coordinateUncertaintyInMeters = 500,
    
    # calc the amount of splits of a sample as a whole number to multiply
    # with average and dilution factor
    split_amount       = if_else(is.na(split_amount), 0, split_amount),
    total_split_amount = 2^split_amount * split_size^-1, # whole number
    total_split_frac   = total_split_amount^-1,
     
    number_ind_sample = eval(other_vars$equation_zoo),
    ind_m3 = number_ind_sample / volume_filt_cubic_m,

    # get aphia ID from end of scientificNameID
    aphiaID = str_extract(scientificNameID, "\\d+"),
    aphiaID = as.numeric(aphiaID)
  ) %>%
  left_join(.,
    aphia_id,
    by = c(
      "taxa_orig" = "taxa_orig",
      "aphiaID"   = "aphia_id"
    )
  )
```

# Save Merged Data
```{r save-data}
save_csv(
    .data         = taxa_original,
    save_location = here("data", "processed", "pre_obis"),
    save_name     = "zoo_data_pre_obis_merg",
    overwrite     = FALSE,
    verbose       = TRUE
)

if (FALSE && exists(cloud_dir)) {
  here("data", "processed", "pre_obis") %>%
  dir_ls() %>%
  last_mod() %>%
  file_copy(
      path = .,
      new_path  = here(cloud_dir, "processed", "zoo_data_pre_obis_merg.csv"),
      overwrite = TRUE
    )
}
```

# Create data sheet for Enrique Montes
```{r}
taxa_original %>%
    names()

temp <-
    taxa_original %>%
    mutate(
        key = glue("{cruise_id}:{site}:{mesh}"),
        .before = 1
    ) %>%
    arrange(date_time)
    
meta <- 
    temp %>%
    select(key, cruise_id, site, mesh, date_time,
           lon_in, lat_in, volume_filt_cubic_m) %>%
    distinct()
    
dat <- 
    temp %>%
    select(key, taxa_orig, scientificName, aphiaID, lifeStage, ind_m3, 
           number_ind_sample)
```

```{r}

wb <-
  createWorkbook(
    creator = "Sebastian Di Geronimo",
    title   = "South Florida Zooplankton Net Tows"
  )

# add sheets
addWorksheet(
  wb        = wb,
  sheetName = "metadata"
)

addWorksheet(
  wb        = wb,
  sheetName = "abundances"
)

# add data to sheets
writeData(
  wb    = wb,
  sheet = "metadata",
  x     = meta
)

writeData(
  wb    = wb,
  sheet = "abundances",
  x     = dat
)

openXL(wb)
```

```{r}
dir_create(here("data", "processed", "merged"))
merged_file <- 
    file_expr(loc = here("data", "processed", "merged"),
              file_base = "zooplankton_fl_keys",
              exts = "xlsx")[[2]]
if (FALSE) {
    saveWorkbook(wb,
                 file = eval(merged_file))
} else {
    cli::cli_alert_warning("Not saving final workbook!")
}
```

```{r}
meta <- 
    here(cloud_dir, "cruise_logsheets") %>%
    dir_ls(type = "file") %>%
    last_mod() %>%
    read_csv(show_col_types = FALSE) %>%
    
    mutate(locationID      = case_when(
                str_detect(station, "Mol")  ~ "MR",
                str_detect(station, "Loo")  ~ "LK",
                str_detect(station, "West") ~ "WS",
                str_detect(station, "9B")   ~ "9B",
                .default = station), 
           .after = station)

dat <- 
    here(cloud_dir, "processed") %>%
    dir_ls(type = "file") %>%
    last_mod() %>%
    read_csv(show_col_types = FALSE)


merged_master <- 
    left_join(dat, 
          meta, 
          by = join_by(cruise_id, 
                       site == locationID, 
                       mesh == mesh_size_um))

naniar::gg_miss_case(merged_master)
```

