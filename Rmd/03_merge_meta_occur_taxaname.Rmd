---
title: "Merge Metadata, Ocurrence and AphiaID"
author: "Sebastian DiGeronimo"
date: "2023-03-07"
output: html_document
---

TODO: check how to save UTF-8 encoding on csv files 
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
librarian::shelf(
    librarian, ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr,
    forcats, lubridate, glue, fs, magrittr, here,
    # broom # optional
    
    # additional
    openxlsx
)

library("conflicted")

conflicts_prefer(dplyr::filter,
                 dplyr::select)

# library("readxl")
# library("hablar") # might be useful when needing to fix names
# library("worrms")
# library("geosphere")


# obistools::event_fields()
# source(here::here("scripts","log_file_changes.R"))
# startup("log_edits")
# log_add()
```

```{r other-vars}
other_vars <- list()

# ---- recorded by info
other_vars$identifier       <- "Natalia Lopez Figueroa"
other_vars$identifier_orcid <- "https://orcid.org/0000-0002-7527-0481"

# ---- max depth per site
other_vars$max_depth <-
  tribble(
    ~site, ~maximumDepthInMeters,
     "MR",                  36.5,
     "LK",                  40.5,
     "WS",                    23,
     "54",                     3,
     "57",                     5,
     "9B",                    31
  )

# ---- set variables for calculation
other_vars$inpeller <- 26873 / 999999 # starting oct 2022 for 200/500 um
# net_area <- pi * 0.5^2

# Note: jamie dillution / pippete_vol_m_l is the same as nat dillution_factor
# 
# total num individual (sum all aliquots)  * vol sample water (known vol) / counted aliquot * split size / folson (mL per aliquot)
# 
# 
# mean = total num / counted aliquots 
# dilution factor = vol sample water / folson 
# 
which_one <- "nat"
other_vars$equation_zoo <- switch(
    which_one,
    "nat"    = expression(dillution_factor * mean * 2 ^ (splits_analyzed)),
    "nat2"   = expression(dillution_factor * mean * (splits_analyzed * split_size)^-1),
    "nat3"   = expression(dillution_factor * mean * (splits_analyzed)^-1),
    "jamie"  = expression((dillution / pipette_vol_m_l) * mean * splits_analyzed ),
    "jamie2" = expression(dillution * mean)
)
rm(which_one)
```


# Load functions if not already
This will load the functions needed if not already contain in the search path.
```{r load-functions}
source(here("scripts", "attach_funcs.R"))
func_attach()
rm(func_attach)
```

# Load data for OBIS conversion
```{r load-data}
aphia_id <- 
    here("data", "metadata") %>%
    dir_ls(regexp = "aphia_additional")

if (is_empty(aphia_id) | FALSE) {
  cli_alert_info("Creating a additional aphia ID file.")

  # ---- load aphia id
  aphia_id <-
    dir_ls(
      path = here("data", "metadata", "aphia_id"),
      regexp = "^[^~]*(aphia)+.*\\.csv$"
    ) %>%
    last_mod(.) %>%
    read_csv(show_col_types = FALSE) %>%
    mutate(
      aphiaID = str_extract(scientificNameID, "\\d+"),
      aphiaID = as.numeric(aphiaID),
      info = map(
        aphiaID,
        ~ tryCatch(
          {
            worrms::wm_record(.x)
          },
          error = function(e) {
            return(NULL)
          }
        )
      )
    ) %>%
    unnest(info, names_repair = janitor::make_clean_names) %>%
    select(!contains("_2"), -c(2:6))


  meta_file <-
    (file_expr(
      loc = here("data", "metadata"),
      file_base = "aphia_additional"
    ))[[2]] %>%
    eval()

  cli_alert_info("File name: {.file {basename(meta_file)}}")

  write_csv(aphia_id,
    file = meta_file,
    na   = ""
  )
} else {
  aphia_id <-
    aphia_id %>%
    last_mod()

  cli_alert_info("Loading file: {.file {basename(aphia_id)}}")

  aphia_id <- 
      read_csv(
          aphia_id,
          show_col_types = FALSE
  )
}
```

```{r load-data}
# ---- load metadata
meta_df <-
    dir_ls(
        path   = here("data", "metadata", "cruise_logsheets"),
        regexp = "^[^~]*(meta_)+.*\\.csv$") %>%
    last_mod(.) %>%
    read_csv(show_col_types = FALSE) %>%
    
    # this is when Natalia LF started analysis
    filter(date >= date("2017-06-18")) %>%
    
    mutate(locationID      = case_when(
                str_detect(station, "Mol")  ~ "MR",
                str_detect(station, "Loo")  ~ "LK",
                str_detect(station, "West") ~ "WS",
                str_detect(station, "9B")   ~ "9B",
                .default = station), 
           .after = station) %>%
    mutate(
        # Note: flowmeter is MF315
        net_size            = 0.5,
        net_area            = pi * 0.5^2,
        distance_m_2        = (flowmeter_out - flowmeter_in) * inpeller_constant,
        tow_speed_m_sec     = distance_m_2 / (tow_time_min * 60),
        # tow_speed_cm_sec     = distance_m_2 * 100 / (tow_time_min * 60),
        volume_filt_cubic_m = net_area/4 * distance_m_2,
        split_size          = map_dbl(split_size, function(x) {
                                    out <- tryCatch({
                                    eval(parse(text = x))}, 
                                    error = function(e) {NA_integer_})}) 
       ) 

# ---- load zooplankton data
taxa_files <-
    dir_ls(
        path = here("data", "processed"),
        regexp = "all_merged_processed") %>%
    last_mod(.) %>%
    map_dfr(~ read_csv(., show_col_types = FALSE)) %>%
    mutate(
        # ONLY USED IF:
        # need to fix info from a specific cruise ID
        # - uncomment and replace "" with the cruise ID and new cruise ID
        # cruise_id = case_when(
        #     str_detect(cruise_id, "") ~ "", # incase other become problematic
        #     .default = cruise_id
        #     ),
        site      = str_replace(site, "MK", "MR"),
        # lifeStage = case_when(
        #     is.na(lifeStage) ~ "adult",
        #     TRUE ~ lifeStage),
        splits_analyzed = case_when(
            is.na(splits_analyzed) | splits_analyzed == 0 ~ 1,
            .default = splits_analyzed
        )
    ) %>%
  select(-mean_ind_dil_factor) %>%
  rowwise(aliquot_1:aliquot_3) %>%
  mutate(
      individualCount = sum(aliquot_1, 
                            aliquot_2, 
                            aliquot_3, 
                            na.rm = TRUE),
      .before = aliquot_1
  ) %>%
  ungroup()

# ---- merge metadata and taxa data
taxa_original <-
  taxa_files %>%
  left_join(
    meta_df,
    by = c(
      "cruise_id",
      "site" = "locationID",
      "mesh" = "mesh_size_um"
    )
  ) %>%
  
  # add max depth
  left_join(other_vars$max_depth) %>%
    
  mutate(
    # add min depth and uncertainty
    minimumDepthInMeters          = 0,
    coordinateUncertaintyInMeters = 500,
    
    # split_amount = 0.5, # splits from cruise
    splits_analyzed = if_else(
      is.na(splits_analyzed), 
      0,
      splits_analyzed
    ),
    number_ind_sample = eval(other_vars$equation_zoo),
    ind_m3 = number_ind_sample / volume_filt_cubic_m,

    # get aphia ID from end of scientificNameID
    aphiaID = str_extract(scientificNameID, "\\d+"),
    aphiaID = as.numeric(aphiaID)
  ) %>%
  left_join(.,
    aphia_id,
    by = c(
      "taxa_orig" = "taxa_orig",
      "aphiaID"   = "aphia_id"
    )
  )
```

# Save Merged Data
```{r save-data}
save_csv(
    .data         = taxa_original,
    save_location = here("data", "processed", "pre_obis"),
    save_name     = "zoo_data_pre_obis_merg",
    overwrite     = FALSE,
    verbose       = TRUE
)

if (FALSE && exists(cloud_dir)) {
  here("data", "processed", "pre_obis") %>%
    dir_ls() %>%
    last_mod() %>%
    tibble(old_name = .) %>%
    mutate(
      new_name = basename(old_name),
      new_name = str_remove(new_name, "_\\d{8}.*[^\\.csv]")
    ) %$%
    walk2(
      .x = old_name,
      .y = new_name,
      ~ file_copy(
        path = .x,
        new_path = here(cloud_dir, "processed", .y),
        overwrite = TRUE
      )
    )
}
```

# Create data sheet for Enrique Montes
```{r}
taxa_original %>%
    names()

temp <-
    taxa_original %>%
    mutate(
        key = glue("{cruise_id}:{site}:{mesh}"),
        .before = 1
    ) %>%
    arrange(date_time)
    
meta <- 
    temp %>%
    select(key, cruise_id, site, mesh, date_time,
           lon_in, lat_in, volume_filt_cubic_m) %>%
    distinct()
    
dat <- 
    temp %>%
    select(key, taxa_orig, scientificName, aphiaID, lifeStage, ind_m3, 
           number_ind_sample)
```

```{r}

wb <-
  createWorkbook(
    creator = "Sebastian Di Geronimo",
    title   = "South Florida Zooplankton Net Tows"
  )

# add sheets
addWorksheet(
  wb        = wb,
  sheetName = "metadata"
)

addWorksheet(
  wb        = wb,
  sheetName = "abundances"
)

# add data to sheets
writeData(
  wb    = wb,
  sheet = "metadata",
  x     = meta
)

writeData(
  wb    = wb,
  sheet = "abundances",
  x     = dat
)

openXL(wb)
```

```{r}
dir_create(here("data", "processed", "merged"))
merged_file <- 
    file_expr(loc = here("data", "processed", "merged"),
              file_base = "zooplankton_fl_keys",
              exts = "xlsx")[[2]]
if (FALSE) {
    saveWorkbook(wb,
                 file = eval(merged_file))
} else {
    cli::cli_alert_warning("Not saving final workbook!")
}
```

```{r}
meta <- 
    here(cloud_dir, "cruise_logsheets") %>%
    dir_ls(type = "file") %>%
    last_mod() %>%
    read_csv(show_col_types = FALSE) %>%
    
    mutate(locationID      = case_when(
                str_detect(station, "Mol")  ~ "MR",
                str_detect(station, "Loo")  ~ "LK",
                str_detect(station, "West") ~ "WS",
                str_detect(station, "9B")   ~ "9B",
                .default = station), 
           .after = station)

dat <- 
    here(cloud_dir, "processed") %>%
    dir_ls(type = "file") %>%
    last_mod() %>%
    read_csv(show_col_types = FALSE)


merged_master <- 
    left_join(dat, 
          meta, 
          by = join_by(cruise_id, 
                       site == locationID, 
                       mesh == mesh_size_um))

naniar::gg_miss_case(merged_master)
```

