---
title: "OBIS Event Creations"
author: "Sebastian DiGeronimo"
date: '2022-09-02'
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library("readxl")
library("hablar") # might be useful when needing to fix names
library("worrms")

library("ggplot2")
library("tibble")
library("tidyr")
library("readr")
library("purrr")
library("dplyr")
library("stringr")
library("forcats")
library("lubridate")
library("glue")
library("fs")
library("magrittr")
library("broom") # optional

library("tidyverse")
library("ggforce")
library("geosphere")
library("vroom")
library("plotly")

obistools::event_fields()
# source(here::here("scripts","log_file_changes.R"))
# startup("log_edits")
# log_add()
```

Workflow:
<https://github.com/ioos/bio_data_guide/tree/main/OBIS_data_tiers>
Needed: 3 things 1. Event 2. Occurances 3. Measurement of Fact (MOF)

1.  Read and format metadata spreadsheets
    -   convert to DarwinCore format

MOF = measurement of fact

Event - where/when took place

Occurrence - species info, presence/absence Occurrence MOF - quantity,
other info to measure them

MOF - environmental quantities measured

# not finished yet

occurance = institution_code = "USF_IMaRS" 
collection_code = "compiled_zoo_taxonomy_nlf" 
catalog_number = "20yy_mm_dd" 
occurrenceID = (unique identifier) 
 joining of <urn:catalog> institution code
collection_code catalog_number row number 
join by : 
eventDate = date_time 
decimalLongitude = lon_in 
decimalLatitude = lat_in
scientificName = natalia spreadsheet 
scientificNameID = "TODO: worms lookup", #taxonomy_df["classification"],

occurence = basisOfRecord = "HumanObservation", 
collectionCode = collection_code, 
catalogNumber = catalog_number, 
occurrenceStatus = "present", 
institutionCode = institution_code

extra-stuff mesh_size flowmeter in flowmeter out ship speed inpeller
constant distnace tow speed formalin vol filtered

# Load data for OBIS conversion
```{r load-data}
# ---- set variables for calculation ----
inpeller <- 26873 / 999999
net_area <- pi * 0.5^2

# ---- load metadata, data, and aphiaid ----
meta_df <-
    fs::dir_ls(path = here::here("data", "metadata"),
               regexp = "^[^~]*(meta_)+.*\\.csv$") %>%
    last_mod(.) %>%
    read_csv(show_col_types = FALSE) %>%
    
    # this is where Natalia LF started
    filter(date >= date("2017-06-18")) %>%
    
    mutate(locationID      = case_when(
        str_detect(station, "Mol") ~ "MR",
        str_detect(station, "Loo") ~ "LK",
        str_detect(station, "West") ~ "WS",
        str_detect(station, "9B") ~ "9B",
        TRUE ~ station), 
        .after = station) %>%
    mutate(
        # Note: flowmeter is MF315
        distance_m_2        = (flowmeter_out - flowmeter_in) * inpeller_constant,
        tow_speed_cm_sec    = distance_m_2 * 100 / (tow_time_min * 60),
        volume_filt_cubic_m = net_area * distance_m_2
        ) 

aphia_id <-
    fs::dir_ls(path = here::here("data", "metadata", "aphia_id"),
               regexp = "^[^~]*(aphia)+.*\\.csv$") %>%
    last_mod(.) %>%
    read_csv(show_col_types = FALSE)

# taxa_files <- 
#     fs::dir_ls(path = here::here("data", "processed"),
#                regexp = "um_taxa_merged") 
taxa_files <-
    fs::dir_ls(path = here::here("data", "processed"),
               regexp = "all_merged_processed") 

# ---- fixing issues ----
taxa_matched_merg <-
    taxa_files  %>%
    map_dfr(~ read_csv(., show_col_types = FALSE)) %>%
     mutate(
        cruise_id = case_when(
            str_detect(cruise_id, "006") ~ "WS20006",
            str_detect(cruise_id, "WS118218") ~ "WS18218",
            str_detect(cruise_id, "WS18224") ~ "WS18218",
            str_detect(cruise_id, "WS18284") ~ "WS18285",
            # str_detect(cruise_id, "") ~ "",
            TRUE ~ cruise_id
        ),
        site = str_replace(site, "MK", "MR")
    )  %>%
    left_join(meta_df, 
              by = c("cruise_id", "site" = "locationID", 
                     "mesh" = "mesh_size_um")) %>%
    mutate(
        # split_amount = 0.5, # doesnt matter I guess
        number_ind_sample = dillution * mean * 2^(splits_analyzed),
        ind_m3 = number_ind_sample / volume_filt_cubic_m,
        aphiaID = if_else(!is.na(scientificNameID), 
                          str_split(scientificNameID, ":", simplify = T)[, 5],
                          NA_character_)
        )
```

```{r}
svDialogs::dlg_list(c("idad", "dfasdklfj"))
svDialogs::dlg_input("heres and input")
taxa_matched_merg %$% 
    unique(taxa_orig) %>%
match_taxa_fix()

```

# Occurrence Bare Minimum:

```{r obis_file}
data.table::data.table(
      occurrenceID = c("my-dataset-0001c29"),
   decimalLatitude = c(-87.7575),
  decimalLongitude = c(24.4727),
    scientificName = c("Sparisoma aurofrenatum"),
  scientificNameID = c("urn:lsid:ipni.org:names:37829-1:1.3"),
  occurrenceStatus = c("present"),
     basisOfRecord = c("HumanObservation"),
         datasetID = c("my-dataset-tylar-2020-01-08-123456"),
         eventDate = c("2010-01-03T13:44Z")
)

# split sizes?
meta_df %>%
    select(split_size) %>%
    mutate(
        split_zie = eval(parse(split_size))
    )
map(meta_df$split_size, function(x) {
    out <- tryCatch({
        eval(parse(text = x))
    }, error = function(e) {
        message(sprintf("Error in %s: %s", deparse(e[["call"]]), e[["message"]]))
        NA_integer_
    })
    return(out)
})
```

```{r record-level}
rcd_lvl <-
    # meta_df %>%
    taxa_matched_merg  %>%
    dplyr::transmute(
        type                = "Event",
        modified            = Sys.Date(),
        language            = "en",
        license             = "http://creativecommons.org/publicdomain/zero/1.0/legalcode",
        institutionCode     = "USF_IMaRS",
        datasetID           = "FKNMS_MBON",
        # which one?
        datasetName         = "MBON Florida Keys National Marine Sanctuary",
        basisOfRecord       = "HumanObservation",
        
        # TODO: will check on this
        # informationWithheld = "collector & analyst identities withheld",
        recordedBy          = "Natalia Lopez Figueroa", # check these
        recordedByID        = "NEED to include ORCID", # check these
    )
```
```{r event}
event <-
    # meta_df %>%
    # TODO: should probably be just meta_df
    taxa_matched_merg  %>%
    dplyr::transmute(
        # catalogNumber   = row_number(),
        locationID      = case_when(
            str_detect(station, "Mol") ~ "MR",
            str_detect(station, "Loo") ~ "LK",
            str_detect(station, "West") ~ "WS",
            TRUE ~ station
        ),
        parentEventID      = "MBON_zooplankton",
        datasetID          = glue("{parentEventID}:{cruise_id}"),
        # will be {cruise_id}
        eventDateTime      = format(date_time, "%Y-%m-%dT%H:%m:%S%z"),
        # eventID - something like cruiseID:stationID:meshsize
        # {{mbon} : cruise} : stn : mesh : date_time
        eventID            = glue(
            # "{datasetID}:stn_{locationID}:{mesh_size_um}_um:{eventDateTime}"
                        "{datasetID}:stn_{locationID}:{mesh}_um:{eventDateTime}"
        )  %>% str_remove("\\+0000"),
        # will be {cruise_id}
        # fieldNumber        = glue("cruiseID {station} {mesh_size_um}"),
        fieldNumber        = glue("{cruise_id} {station} {mesh}"),
        eventDate          = date(date_time),
        eventTime          = format(date_time, "%H:%m:%S%z"),
        year               = year(date_time),
        month              = month(date_time),
        day                = day(date_time),
        # samplingProtocol = paste(mesh_size_um, "mesh size (um)"),
        samplingProtocol   = glue(
            "{mesh} mesh size (um) - bongo nets | http://drs.nio.org/drs/handle/2264/95"
        ),
        
        # TODO: habitat NERC vocab
        habitat            = "near reef",
        sampleSizeValue    = "something like distance traveled or volume filtered?",
        # volume filtered
        sampleSizeUnit     = "would equal metre or volume?" # volume
    ) %>%
     distinct() %>%
     mutate(
         # TODO: save for the end when combining all same sizes together
         catalogNumber   = row_number(),
         .before = everything()
     )
```
```{r location}
location <-
    # meta_df %>%
    taxa_matched_merg %>%
    dplyr::transmute(
        eventID           = event$eventID,
        decimalLatitude   = lat_in,
        decimalLongitude  = lon_in,
        higherGeographyID = case_when(
            str_detect(station, "Mol|Looe|West") ~ "http://vocab.getty.edu/tgn/7030258",
            str_detect(station, "5") ~ "http://vocab.getty.edu/tgn/1101513",
            TRUE ~ "Not Found"
        ),
        higherGeography   = "North America | United States | Florida",
        continent         = "North America",
        country           = "United States",
        countryCode       = "US",
        stateProvince     = "Florida",
        geodeticDatum     = "EPSG:4326",
        georeferencedBy   = "NOAA AOML | USF IMaRS | RSMAS R/V Walton Smith"
    ) %>%
    distinct()

```
```{r occurence}
# occur <- 
# Doesnt exists yet!!!
taxa_matched_merg %>%
   dplyr::transmute(
       scientificName,
       lifeStage = if_else(!is.na(lifeStage), lifeStage, "other"),
       eventID                  = event$eventID,
       occurrenceID             = glue("{eventID}:{aphiaID}:{lifeStage}"),
       recordedBy               = "Natlia Lopez Figueroa", # check these
       recordedByID             = "Natalia LP ORCID", # check these
       
       # IDk which of these is accurate
       individualCount          = "# Ind/Sample",
       organismQuantity         = "Ind/m^3", # or `# Ind/Sample` and leave out individualCount?
       organismQuantityType     = "Individuals per cubic metre", # or
       measurementType          = "Number per cubic metre", # MOF? should also include ind/sample?
       measurementUnitID        = "http://vocab.nerc.ac.uk/collection/P06/current/UPMM/", # MOF?
       
       # ---- check this information ---- 
       lifeStage                = lifeStage,
       establishmentMeans       = "native | uncertain",
       occurrenceStatus         = "present", # need to filter ind/sample > 0
       preparations             = "formalin | ethanol",
       scientificNameID         = scientificNameID, # like urn:lsid:ipni.org:names:37829-1:1.3
       basisOfRecord            = "HumanObservation", 
       datasetID                = event$datasetID,
       eventDate                = date(date_time),
       identificationReferences = "WoRMS",
       verbatimIdentification   = taxa
       ) 
```
```{r MoF}
# ---- Measurement or Fact ----
# MoF https://github.com/ioos/bio_data_guide/blob/main/datasets/WBTS_MBON/IOOS%20DMAC%20DataToDwC_Notebook_event.ipynb
tibble::tribble(
              ~Origin.Term,                                       ~`measurementTypeID`,       ~URI,
                "Net_Type",                                             "plankton net",       "22",
               "Mesh_Size",                                   "Sampling net mesh size", "Q0100015",
               "NET_DEPTH",       "Depth (spatial coordinate) of sampling event start", "DXPHPRST",
                 "COMMENT",            "N/A (mapped to measurementRemark field above)",      "N/A",
       "Plankton_Net_Area",                    "Sampling device aperture surface area", "Q0100017",
         "Volume_Filtered",                                                   "Volume",      "VOL",
            "Sample_Split", "N/A (information added to measurementRemark field above)",      "N/A",
       # "Sample_Dry_Weight",                                       "Dry weight biomass", "ODRYBM01",
                # "DW_G_M_2",                                       "Dry weight biomass", "ODRYBM01",
         "Dilution_Factor",                                                      "???",      "???",
    "TOTAL_DILFACTOR_CFIN",                                                      "???",      "???"
    )

# ---- example MoF ---- 
column_mappings <- 
    tibble::tribble(
                        ~types,                    ~uri,     ~unit,             ~unitID, ~accuracy,                        ~type,         ~measurementMethod,
                    "Net_Type",       "L05/current/22/",        NA,                  NA,        NA,                   "net type",                         NA,
                   "Mesh_Size", "Q01/current/Q0100015/", "microns", "P06/current/UMIC/",        NA,                  "mesh size",                         NA,
                   "NET_DEPTH", "P01/current/DXPHPRST/",       "m", "P06/current/UPAA/",        NA,                  "net depth",                         NA,
           "Plankton_Net_Area", "Q01/current/Q0100017/",      "m2", "P06/current/UPAA/",        NA,          "plankton net area",                         NA,
             "Volume_Filtered",      "P25/current/VOL/",      "m3", "P06/current/UPAA/",        NA,            "volume filtered", "geometrically determined",
             "Dilution_Factor",                      NA,      "ml", "P06/current/VVML/",        NA,            "dilution factor",                         NA,
        "TOTAL_DILFACTOR_CFIN",                      NA,      "ml", "P06/current/VVML/",        NA, "Total Dilution factor CFIN",                         NA,
                "Sample_Split",                      NA, "decimal", "P06/current/UPCT/",        NA,               "sample split",          "Folsom Splitter"
        )

clipr::write_clip(calc)
tibble::tribble(
    ~date_collected, ~sample_id, ~filtered_volume_m3, ~pipette_vol_m_l, ~dillution, ~dillution_factor, ~mesh, ~date_analyzed, ~split_amount,
       "2015-11-16",  "MR 1115",                 97L,               5L,       250L,               50L,  500L,   "2021-07-23",            2L
    )

# http://vocab.nerc.ac.uk/collection/L05/current/22/
# might need to pivot wider first, then pivot longer



mof <- 
    occur %>%
    dplyr::transmute(
        occurrenceID      = occur$occurrenceID,
        eventID           = event$eventID,
        measurementType   = "Relative abundance",
        measurementID     = "http://vocab.nerc.ac.uk/collection/S06/current/S0600020/",
        
        measurementValue  = `data$ind/m^3`,
        measurementType   = "Number per cubic metre", # MOF?
        measurementID     = "http://vocab.nerc.ac.uk/collection/S06/current/S0600002/",
        measurementUnitID = "http://vocab.nerc.ac.uk/collection/P06/current/UPMM/", # MOF?
    )


```