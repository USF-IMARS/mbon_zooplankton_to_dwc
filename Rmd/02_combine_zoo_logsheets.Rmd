---
title: "Combine Zooplankton Logsheet"
author: "Sebastian DiGeronimo"
date: '2022-07-08'
output: html_document
---

# Load Packages
This loads any packages that are not contained in the .Rprofile.
```{r setup}
librarian::shelf(
    readxl, hablar, worrms, ggforce, geosphere, vroom, plotly,
    quiet = TRUE
    )
```

# Load functions if not already
This will load the functions needed if not already contain in the search path.
```{r load-functions}
source(here("scripts", "attach_funcs.R"))
func_attach()
rm(func_attach)
```

# Load cruise metadata files from `data/metadata/` 
1. Parse all file paths
- Will ignore files with `~$` in front because these are usually temporary files that are opened
2. Remove files that would not contain zooplankton metadata
3. Loop through each file and look for sheet with `zooplankton` in the name 
```{r read-sheets, include=FALSE}
if (!exists("main_cloud_dir")) main_cloud_dir <- rstudioapi::selectDirectory()

# select all cruise files
metadata_paths <-
  main_cloud_dir %>%
  dir_ls(type = "directory") %>%
  dir_ls(regexp = "metadata", recurse = 1) %>%
  dir_ls(regexp = "\\.xlsx$") %T>% 
  {
    basename(.) %>%
    print()    
  } %>%
  str_subset(
     paste(
    "(?i)ignore|~|filtA|All_cruise|BB3|Digna|Schedule|Kelble|apad|cdom",
    "cruisetrack|noaa|sample_id_|zooplankton|collected|net_tow|edna",
    "creation_blank|core|mwreu_",
       sep = "|"), 
    negate = TRUE) %T>% 
  {
    basename(.) %>%
    print()    
  } 



meta_skips <-
  metadata_paths %>%
  tibble(file = .) %>%
  mutate(
    base = basename(file),
    cruise_id = str_extract(file, "(WS|SV|HG|H|WB)\\d{4,5}"),
    sheets =
    # read all files for sheet names that match zooplankton and Sheet2
      map_chr(
        .x = file,
        \(.x) {
          sht_all <- readxl::excel_sheets(.x) 
          sht <- str_subset(sht_all, "(?i)zooplankton", negate = FALSE)
          
          # if (is_empty(sht)) sht <- str_subset(sht_all, "(?i)sheet2")
          # if (is_empty(sht)) sht <- str_subset(sht_all, "(?i)sampling")

          if (is_empty(sht)) sht <- NA_character_

          sht
        }
      )
  ) %>%
  filter(!is.na(sheets)) %>%
  mutate(
    skips =
      map2_int(
        .x = file,
        .y = sheets,
        function(.x, .y) {
          cat(basename(.x), "\n")

          # find number of rows to skip to get to metadata
          skips <- 
            readxl::read_xlsx(
              .x,
              range = cell_cols("A"),
              col_names = FALSE,
              sheet = .y,
              .name_repair = "unique_quiet"
            ) %>%
            pull(1) %>%
            str_which("(?i)zooplankton sampling")

          # if skips can't find a number, will make NA
          if (is_empty(skips)) skips <- NA_integer_

          skips
        }
      )
  ) %T>%
  print()
```

```{r}
meta_skips %>%
  filter(str_detect(sheets, "sampling|Sheet2")) %>%
  pull(file) %>%
  map(~ shell.exec(.x))
```


```{r load-logsheets}
meta <-
  meta_skips %>%
  # slice(1) %>%
  filter(!is.na(skips)) %>%
  mutate(
    data = pmap(
      .l = list(.x = file, .y = skips, .z = sheets, .c = cruise_id),
      function(.x, .y, .z, .c) {
        cli_alert_info(.c)

        # read sheets
        temp <-
          readxl::read_xlsx(
            .x,
            sheet = .z,
            skip = .y,
            .name_repair = janitor::make_clean_names,
            na = c("skip", "Did not collect", "not recorded", "na")
          ) %>%
          # remove rows that are fully empty
          janitor::remove_empty(which = "rows") %>%
          # remove formalin_vol_ml column
          select(-contains("formalin_vol_ml")) %>%
          # fill station names down if not there
          tidyr::fill(station) %>%
          # drop rows when flowmeter_out = NA
          drop_na(flowmeter_out) %>%
          # fill down if lat, lon, date, or time is not there
          tidyr::fill(contains(c("lat", "lon", "date", "time"))) %>%
          mutate(
            across(
              contains("local_time_est"),
              \(.x) {
                x <-
                  tryCatch(
                    {
                      ymd_hms(.x, tz = "est") %>%
                        format(
                          format = "%H:%M:%S",
                          tz = "utc"
                        ) %>%
                        hms::as_hms()
                      #   hms::as_hms(
                      #   format(
                      #     .x,
                      #     format = "%H:%M:%S",
                      #     tz = "utc"
                      #   )
                      # )
                    },
                    warning = function(w) {
                      NA
                    },
                    error = function(e) {
                      NA
                    }
                  )
              }
            )
          ) %>%
          rename("time_gmt" = contains("local_time_est")) %>%
          mutate(
            # convert time if labeled as time_gmt if needed
            across(
              contains(c("time_gmt")),
              \(.x) {
                x <-
                  tryCatch(
                    {
                      hms::as_hms(
                        format(
                          .x,
                          format = "%H:%M:%S",
                          tz = "utc"
                        )
                      )
                    },
                    warning = function(w) {
                      NA
                    },
                    error = function(e) {
                      NA
                    }
                  )
              }
            ),

            # convert time_gmt to date_time if possible with date and
            across(
              contains(c("time_gmt")),
              list(date_time = \(.x) {
                x <-
                  tryCatch(
                    {
                      ymd_hms(paste(date, .x), tz = "utc")
                    },
                    warning = function(w) {
                      NA_POSIXct_
                    },
                    error = function(e) {
                      NA_POSIXct_
                    }
                  )
              }),
              .names = "{.fn}"
            ),
            .after = 4
          )

        # convert lat_in and/or lon_in from deg minutes to decimal
        # degrees if type is str
        if (is.character(temp$lat_in) | is.character(temp$lon_in)) {
          message("Converting lat and lon into decimal degrees.\n")
          temp %<>%
            add_column(
              with(
                .,
                parzer::parse_lon_lat(lon_in, lat_in)
              ),
              .after = "lon_in"
            ) %>%
            select(-lon_in, -lat_in) %>%
            rename(lon_in = lon, lat_in = lat)
        }

        # change name of x to notes if exists
        if (!is.null(temp)) temp <- rename(temp, any_of(c(notes = "x")))

        # check if can retype columns to chr, num, or date
        temp <- tryCatch(
          {
            temp <- hablar::retype(temp, -time_gmt)
          },
          error = function(e) {
            message("Doesn't have a time_gmt column. Returning as is.\n")
            temp
          }
        )

        # specify what to convert types to merge later
        temp <- hablar::convert(
          temp,
          chr(
            ship_speed_knots, split_size,
            station, mesh_size_um
          )
        )

        # if (str_detect(.c, "WS16207")) browser()
        if (is.POSIXct(temp$tow_time_min)) {
          temp <-
            temp %$%
            hms::as_hms(tow_time_min) %>%
            str_split(":", simplify = TRUE) %>%
            as_tibble() %>%
            mutate(
              across(c(V1, V2, V3), \(x) as.numeric(x)),
              time = V1 + V2 / 60
            ) %>%
            select(tow_time_min = time) %>%
            bind_cols(select(temp, -tow_time_min), .)
        }

        temp
      }
    ),
    # filter for sheets without any information
    data_exists =
      map(.x = data, ~ if_else(nrow(.x) > 0, 1, 0)) %>%
        unlist()
  )

```




# Keep record of files that have no data because either didn't collect or didn't write down
```{r filter-no-data}
no_data <- filter(meta, data_exists == 0)
no_data %>%
  pull(data)

```

```{r filter-no-data}
meta_fix <-
  meta %>%
  filter(data_exists > 0) %>%
  select(-data_exists) %>%
  unnest(data) %>%
  select(-file, -sheets, -skips, -contains("local_time_est"), file = base) %>%
  rename(
    "ignore_distance_m" = distance_m, 
    "ignore_tow_speed" = tow_speed
    ) %>%
  relocate(flowmeter_in, .before = flowmeter_out) %>%
  separate_rows(mesh_size_um, sep = "/") %>%
  mutate(
    mesh_size_um = str_trim(mesh_size_um),
    mesh_size_um = retype(mesh_size_um),
    ship_speed_knots = str_remove_all(ship_speed_knots, "~"),
    file = tools::file_path_sans_ext(file),
    split_size = map_dbl(split_size, function(x) {
      out <- tryCatch(
        {
          eval(parse(text = x))
        },
        error = function(e) {
          NA_integer_
        }
      )
    }),
  ) %>%
  arrange(date) %>%
  mutate(
    locationID = case_when(
      str_detect(station, "Mol") ~ "MR",
      str_detect(station, "Loo") ~ "LK",
      str_detect(station, "West") ~ "WS",
      str_detect(station, "9B") ~ "9B",
      .default = station
    ),
    .after = station
  ) %>%
  mutate(
    # Note: flowmeter is MF315
    net_size = # diameter in m
      case_when(
        mesh_size_um == 64 ~ 0.6,
        mesh_size_um == 200 ~ 0.5,
        mesh_size_um == 500 ~ 0.5
      ),
    net_area            = pi * (net_size / 2)^2, # m^2
    flowmeter_diff      = flowmeter_out - flowmeter_in,
    distance_m          = flowmeter_diff * inpeller_constant, # m
    tow_speed_m_sec     = distance_m / (tow_time_min * 60), # m s^-1
    volume_filt_cubic_m = net_area * distance_m, # m^3

    # Fix Latitude and Longitude
    # Some files had mixed up lat and lon and some have lon at a positive
    # decimal degree and should only be negative.
    lon_in = case_when(
      str_detect(locationID, "MR") ~ -80.38000,
      str_detect(locationID, "WS") ~ -81.71700,
      str_detect(locationID, "LK") ~ -81.41300,
      str_detect(locationID, "57") ~ -81.26500,
      str_detect(locationID, "54") ~ -81.15283,
      .default = lon_in
    ),
    lat_in = case_when(
      str_detect(locationID, "MR") ~ 25.01000,
      str_detect(locationID, "WS") ~ 24.47800,
      str_detect(locationID, "LK") ~ 24.53800,
      str_detect(locationID, "57") ~ 25.35167,
      str_detect(locationID, "54") ~ 25.34533,
      .default = lat_in
    )
  ) %>%
  relocate(flowmeter_diff, .after = flowmeter_out) %>%
  relocate(contains("ignore"), .after = last_col()) %>%
  relocate(date_time, date, time_gmt, .after = mesh_size_um) %T>% 
  print()

```




# Save merged metadata
```{r save}
overwrite <- FALSE
overwrite <- TRUE

# ---- write metadata to file ----
save_csv(
  .data          = meta_fix,        
  save_location  = here("data", "metadata", "cruise_logsheets"),
  save_name      = "meta_combined",
  overwrite      = overwrite,
  verbose        = TRUE,
  time_stamp_fmt = "%Y%m%d_%H%M%S",
  utf_8          = FALSE
)

# copy to cloud
if (FALSE) {
  here("data", "metadata", "cruise_logsheets") %>%
  dir_ls(regexp = "meta_combined") %>%
  last_mod() %>% 
  file_copy(
      here(cloud_dir, "cruise_logsheets", "meta_combined.csv"),
      overwrite = TRUE
  )
}

# for NOAA AOML
if (FALSE) {
  meta_fix %>%
    relocate(notes, .after = volume_filt_cubic_m) %>%
    openxlsx2::write_xlsx(
      .,
      here("data", "metadata", "usf_imars_zooplankton_log_sheet.xlsx"),
      na.strings = "",
      overwrite = overwrite
    )
}
```


# Digitized Zooplankton Net Tows

File: `zooplankton_net_tows.xlsx`

A pdf from NOAA AOML with the original hand written logs was digitized by 
Lilly Verrill and Mia Poage in summer 2023.

This file contains:
- cruise_id
- date
- station
- net_size (i.e. mesh size)
- time
- time_zone (b/c was recorded in EDT)
- flowmeter_in	
- latitude	
- longitude	
- tow_time (as time and needs to be converted to number)
- flowmeter_out	
- notes



```{r}
noaa_zoo_pdf <-
  here(cloud_dir, "../miscellaneous_tasks") %>%
  dir_ls(regexp = "zooplankton_net_tows.xlsx") %>%
  str_subset("~\\$", negate = TRUE) %>%
  read_excel() %T>%
  print() %>%
  mutate(
    .before = time,
    time = str_remove(time, "\\d{4}-\\d{2}-\\d{2}\\s"),
    date_time =
      pmap(
        list(
          date,
          time,
          time_zone
        ),
        \(.date, .time, .timezone) {
          # print(.timezone)

          if (is.na(.timezone)) {
            return(lubridate::NA_POSIXct_)
          }
          .timezone <- if_else(.timezone == "EDT", "EST", .timezone)
          as_datetime(paste(.date, .time), tz = .timezone)
        }
      ),
  ) %>%
  unnest(date_time) %>%
  mutate(
    date      = as_date(date_time),
    time      = hms::as_hms(date_time),
    
    # lat/lon fix if has space
    longitude = paste(longitude, "W"),
    latitude  = parzer::parse_lat(latitude),
    longitude = parzer::parse_lon(longitude),
    
    
    
  ) %>%
  select(-time_zone) %>%
  relocate(date, .after = date_time) %>%
  mutate(
    tow_time2 = hms::as_hms(tow_time),
    tow_time2 = str_split(tow_time2, ":")
  ) %>%
  unnest_wider(tow_time2, names_sep = "_") %>%
  mutate(
    across(contains("tow_time2"), as.numeric),  
    tow_time = tow_time2_1 + tow_time2_2/ 60
  ) %>%
  select(-contains("tow_time2"))

```

## Rename and Add Calculations to Digitized Zooplantkon Net Tow

Renaming columns:
New                Old
"mesh_size_um" = net_size
"tow_time_min" = tow_time
"locationID"   = station
"lon_in"       = longitude
"lat_in"       = latitude

Add Columns:
inpeller_constant
net_size (as the diameter of the net in meters)

Calculations:
net_area        = pi * (net_size/2)^2)
flowmeter_diff  = flow out - flow in
distance_m      = flowmeter_diff * inpeller_constant
tow_speed_m_sec = distance_m / (tow_time_min * 60)


```{r}
noaa_zoo_metadata <-
  noaa_zoo_pdf %>%
  rename(
    "mesh_size_um" = net_size,
    "tow_time_min" = tow_time,
    "locationID"   = station,
    "lon_in"       = longitude,
    "lat_in"       = latitude
  ) %>%
  mutate(
    inpeller_constant = 0.245,
    # Note: flowmeter is MF315
    net_size = # diameter in m
      case_when(
        mesh_size_um == 64 ~ 0.6,
        mesh_size_um == 200 ~ 0.5,
        mesh_size_um == 500 ~ 0.5
      ),
    net_area            = pi * (net_size / 2)^2, # m^2
    flowmeter_diff      = flowmeter_out - flowmeter_in,
    distance_m          = flowmeter_diff * inpeller_constant, # m
    tow_speed_m_sec     = distance_m / (tow_time_min * 60), # m s^-1
    volume_filt_cubic_m = net_area * distance_m, # m^3

    # Fix Latitude and Longitude
    # Some files had mixed up lat and lon and some have lon at a positive
    # decimal degree and should only be negative.
    lon_in = case_when(
      str_detect(locationID, "MR") ~ -80.38000,
      str_detect(locationID, "WS") ~ -81.71700,
      str_detect(locationID, "LK") ~ -81.41300,
      str_detect(locationID, "57") ~ -81.26500,
      str_detect(locationID, "54") ~ -81.15283,
      .default = lon_in
    ),
    lat_in = case_when(
      str_detect(locationID, "MR") ~ 25.01000,
      str_detect(locationID, "WS") ~ 24.47800,
      str_detect(locationID, "LK") ~ 24.53800,
      str_detect(locationID, "57") ~ 25.35167,
      str_detect(locationID, "54") ~ 25.34533,
      .default = lat_in
    ),
    
    station = case_when(
      str_detect(locationID, "MR") ~ "Molasses Reef",
      str_detect(locationID, "LK") ~ "Looe Key (deep)",
      str_detect(locationID, "WS") ~ "Western Sambo",
      .default = locationID
    )
    
  ) %>%
  relocate(flowmeter_diff, .after = flowmeter_out) %>%
  relocate(contains("ignore"), .after = last_col()) %>%
  relocate(date_time, date, "time_gmt" = time, .after = mesh_size_um)  %>%
  relocate(station, .before = locationID) %T>%
  print()



```
# Save merged NOAA metadata
```{r save}
overwrite <- FALSE
# overwrite <- TRUE

# ---- write metadata to file ----
save_csv(
  .data          = noaa_zoo_metadata,        
  save_location  = here("data", "metadata", "cruise_logsheets"),
  save_name      = "zoo_noaa_meta",
  overwrite      = overwrite,
  verbose        = TRUE,
  time_stamp_fmt = "%Y%m%d_%H%M%S",
  utf_8          = FALSE
)

# copy to cloud
if (FALSE) {
  here("data", "metadata", "cruise_logsheets") %>%
  dir_ls(regexp = "zoo_noaa_meta") %>%
  last_mod() %>% 
  file_copy(
      here(cloud_dir, "cruise_logsheets", "zoo_noaa_meta.csv"),
      overwrite = TRUE
  )
}

# for NOAA AOML
if (FALSE) {
  meta_fix %>%
    relocate(notes, .after = volume_filt_cubic_m) %>%
    openxlsx2::write_xlsx(
      .,
      here("data", "metadata", "usf_imars_zooplankton_log_sheet.xlsx"),
      na.strings = "",
      overwrite = overwrite
    )
}
```


