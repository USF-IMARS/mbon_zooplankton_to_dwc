---
title: "Fix and Convert Previous Zooplankton Data"
author: "Tylar Murray"
date: "2023-04-14"
format: html
---

# 1.0 ---- Setup ----
## 1.1 Load Libraries
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
if (!nzchar(system.file(package = "librarian"))) 
    install.packages("librarian")

librarian::shelf(
    librarian, conflicted, ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr,
    forcats, lubridate, glue, fs, magrittr, here,
    
    # additional
    cli, naniar, Hmisc
)

conflicts_prefer(
    dplyr::filter(), 
    dplyr::select()
    )
```

# 1.1 Make sure location of files is correct
From fix_prev_zoo_data.qmd
- create folders
  - place original data
  - place processed file to load here 
  
In case, you don't have these folder, creates them here
```{r create-folder}
base_dir <- here::here("data", "zoo_pre_2018")

fs::dir_create(here::here(base_dir, "processed"))
fs::dir_create(here::here(base_dir, "orig"))

```

## 1.2 Get File Paths
sp.csv from:
<https://drive.google.com/drive/folders/1IKBqpgOR6-bhJnm3wHwZAzvSYAA24Bsx>
put in "~/data/zoo_pre_2018"

```{r get-data-path}
# get file path to processed data
file_path <-
  here::here(base_dir, "processed") %>%
  fs::dir_ls(
    # regexp  = "zoo_compiled_with_meta",
    regexp  = "zoo_compiled_with_meta_essentials",
    recurse = TRUE
  ) %>%
  # function to check more recent made file to use
  last_mod() %T>% 
  print()

# read `.csv` file created by `fix_prev_zoo_data.Rmd`
dat <-
  file_path %>%
  # "../zoo_compiled_with_meta_20230411.csv" %>%
  
    readr::read_csv(
    file        = .,
    show_col_types = FALSE,
    guess_max   = 10000,
    name_repair = janitor::make_clean_names
  ) %T>% 
  print()

species_path <-
    here::here(base_dir) %>%
    fs::dir_ls(regexp = "sp\\.csv") %T>%
    print()

# read manually created verbatim name map `sp.csv`
species_map <-
  species_path %>%
  # "../sp.csv" %>%

  readr::read_csv(
    file = .,
    show_col_types = FALSE,
    name_repair = janitor::make_clean_names,
    na = c("", "NA", stringi::stri_dup("?", 1:5)) # ignore `?` 1 to 5 times
  ) %T>% 
  print() %>%

  # select only necessary columns
  dplyr::select(
    jaimes_taxa_info,
    contains("sci_name"),
    lifestage
  ) %T>% 
  print()
```

# 2.0 ---- Merge Taxa Info ----

## 2.1 Merge Species Info with Raw Data

```{r load-csv-data}
# map Jaime's species names to WoRMS-readable using `sp.csv`
dat_merg <-
    species_map %>%

  # join on classification columns. `.` is where it gets piped into
  dplyr::left_join(
    mutate(dat, class2 = clasification, .after = clasification),
    .,
    by = join_by("clasification" == "jaimes_taxa_info")
  ) %>%
    
  # construct unique identifier for each row (used for occurrenceID later)
  dplyr::mutate(
    orig_data_row_ID = glue("{sheet_nm}:{clasification}:{lifestage}"),
    .before = 1
  ) %>%
  distinct() %>%
    
  mutate(
    taxa_to_search =
      if_else(
        is.na(sci_name_guessed_by_tylar),
        clasification,
        sci_name_guessed_by_tylar
      )
  ) %T>% 
  print()
```


## 2.2 Check Duplicates for Unique IDs
```{r check-duplicates}
# validate: rows in file must be unique
# janitor::get_dupes(<dataframe name>, <column names to group together>) 

dupes <- janitor::get_dupes(dat_merg, orig_data_row_ID)

# Check if duplicated rows
if (nrow(dupes) > 0) {
    stop(
        glue("There are duplicated rows in merged data! ",
             "(n = {nrow(dupes)})")
        )
} else {
    message("No duplicated `orig_data_row_ID`.")
}

# only flowmeter data differs and one looks unrealistically large.

```

# 3.0 ---- Get AphiaID from WoRMS ----

Using `merge_taxa()` 
check = if want to check unmatched names
viewer = if want to see unmatched taxa using View()

If using full taxonomic that exists, will skip the taxa merge processing

```{r aphia-id}
taxa_full_path <-
  here(base_dir) %>%
  dir_ls(regexp = "taxa_worms_info") %>%
  last_mod() %T>%
  print()

if (length(taxa_full_path) > 0) {
  # read already completed full extended taxa list
  taxa_full <-
    taxa_full_path %>%
    read_csv(show_col_types = FALSE)
} else {
  # will create full extended taxa list
  
  # set location for aphiaID list and the base of the file name
  file_exprs <-
    file_expr(
      loc       = here::here(base_dir),
      file_base = "aphia_taxa_jaime"
    )

  # if need to check updates to taxa sheet
  # check_taxa <-
  #     here::here(cloud_dir, "old_jaime_data") %>%
  #     # dir_ls(regexp = glue("{file_exprs[1]}.*\\.csv$"))
  # load_taxa_list(
  #     .,
  #     file_exprs
  #     ) %>%
  #     filter(!is.na(notes) & str_detect(notes, "added sp")) %T>%
  #     print()
  # check_taxa %$%
  #     merge_taxa(taxa,
  #                .file_expr =  file_expr(
  #     loc       = here::here(base_dir),
  #     file_base = "aphia_taxa_jaime_test2"
  #     ), .recurse = FALSE)

  # update local master taxa sheet
  if (FALSE) {
    master_taxa_list(
      taxa_list  = NULL,
      .cloud_dir = here(cloud_dir, "old_jaime_data"),
      # file_base  = "aphia_taxa",
      where_to   = "local",
      save       = FALSE,
      .file_expr = file_exprs
    )
  }



  file_verbat <-
    here(base_dir) %>%
    dir_ls(regexp = "aphia_taxa_jaime") %>%
    last_mod()

  taxa_base <-
    dat_merg %>%
    # select(taxa_to_search) %>%
    select(clasification) %>%
    distinct() %>%
    merge_taxa(.,
      .file_expr = file_exprs,
      check = FALSE,
      # check      = TRUE,
      use_cloud = FALSE,
      viewer = TRUE,
      file_verbat = file_verbat
    )

  # extended taxa information from WoRRMS
  taxa_full <-
    taxa_base %>%
    mutate(
      aphia_id = if_else(!is.na(scientificNameID),
        str_split(scientificNameID, ":",
          simplify = TRUE
        )[, 5],
        NA_character_
      ) %>%
        as.numeric(.),
      row = row_number(),
      info = pmap(
        list(
          aphia_id,
          taxa_orig,
          row
        ),
        # .progress = TRUE,
        .f =
          (\(.x, .y, .z)
          tryCatch(
            {
              if (.z == 1) {
                cat(sprintf(
                  "\n%-4s | %-6s | names\n%s",
                  "Row", "aphia",
                  stringi::stri_dup("-", 37)
                ))
              }
              cat(sprintf("\n%-4d | %6s | %s | ", .z, .x, .y))
              x <- worrms::wm_record(.x)
              cat(x$scientificname)
              return(x)
            },
            error = function(e) {
              return(NULL)
            }
          ))
      )
    ) %>%
    unnest(info,
      names_repair = janitor::make_clean_names,
      keep_empty = TRUE
    ) %>%
    rename(
      # might be need to go back to old way
      # "scientificName"   = scientific_name
      # "scientificNameID" = scientific_name_id
      "scientificName"   = valid_name,
      "scientificNameID" = lsid
    ) %>%
    mutate(
      scientificNameID = str_replace(
        scientificNameID, ":(\\d+)",
        glue(":{valid_aphia_id}")
      )
    ) %>%
    relocate(scientificName, scientificNameID, valid_aphia_id, .after = lifestage)

  # save updated version of base taxa and full taxa as .csv
  # loc       = here::here(base_dir),
  # file_base = "aphia_taxa_jaime"
  #
  save_csv(
    .data          = taxa_base,
    save_location  = file_exprs$file_loc,
    save_name      = file_exprs$file_base,
    # overwrite      = TRUE, # <-- uncomment to overwrite
    verbose        = TRUE,
    time_stamp_fmt = "%Y%m%d",
    utf_8          = TRUE
  )

  save_csv(
    .data          = taxa_full,
    save_location  = here(base_dir),
    save_name      = "taxa_worms_info",
    # overwrite      = TRUE, # <-- uncomment to overwrite
    verbose        = TRUE,
    time_stamp_fmt = "%Y%m%d",
    utf_8          = TRUE
  )
}

taxa_full
```


```{r aphia-id}


# if (FALSE) {
#   write_excel_csv(
#     x = taxa_base,
#     file =  eval(file_exprs[[2]]),
#     na = ""
#   )
#   write_excel_csv(
#     x = taxa_full,
#     file = here::here(
#       base_dir,
#       glue(
#         "taxa_worms_info",
#         format(Sys.Date(), "_%Y%m%d"),
#         ".csv"
#       )
#     ),
#     na = ""
#   )
# }

# update cloud service directory if have connection through explorer/Finder
if (FALSE && exists("cloud_dir") && !is.na(cloud_dir)) {
  master_taxa_list(
    taxa_list  = taxa_base,
    .cloud_dir = here(cloud_dir, "old_jaime_data"),
    where_to   = "cloud",
    save       = TRUE,
    .file_expr = file_exprs
  )
  master_taxa_list(
    taxa_list  = taxa_full,
    .cloud_dir = here(cloud_dir, "old_jaime_data"),
    where_to   = "cloud",
    save       = TRUE,
    .file_expr = "taxa_worms_info",
    sheet_name = "Full Taxonomic Info"
  )
}
```




```{r aphia-id-merge}
# station max depths
max_depth <-
  list(
    MR = 36.5,
    LK = 40.5,
    WS = 23
  )

max_depth <-
  list(
    MR = c(36.5, -81.41464542, 24.53862793),
    LK = c(40.5, -81.71532752, 24.47605506),
    WS = c(23,   -80.37953386, 25.00607727)
  )

# filter out taxa names not needed
taxa_full_filt <- 
    taxa_full %>%
     filter(
         (str_detect(notes, "remove", negate = TRUE) | is.na(notes)) 
         & !is.na(scientificNameID)
         ) %>%
      select(-aphia_id_2)  
  

dat_merg_tx <-
  # join on classification columns. `.` is where it gets piped into
  left_join(
    dat_merg,
    taxa_full_filt,
    by = c("taxa_to_search" = "taxa_orig")
  ) %>%
  mutate(
    .keep     = "unused",
    .after    = scientificName,
    lifestage = if_else(!is.na(lifestage.x), lifestage.x, lifestage.y)
  ) %T>% 
  print()

# round individual count. we don't know why there are fractional counts. 
dat_merg_tx <- 
    dat_merg_tx %>%
    
    mutate(
    
    # ind_count = round(ind_count),
    # hardcode missing lat lon using averages in water-samples
    lon_in = case_when(
        is.na(lon_in) & str_detect(stn, "LK") ~ max_depth$MR[2],
        is.na(lon_in) & str_detect(stn, "WS") ~ max_depth$LK[2],
        is.na(lon_in) & str_detect(stn, "MR") ~ max_depth$WS[2],
        .default = lon_in
    ),
    lat_in = case_when(
        is.na(lat_in) & str_detect(stn, "LK") ~ max_depth$MR[3],
        is.na(lat_in) & str_detect(stn, "WS") ~ max_depth$LK[3],
        is.na(lat_in) & str_detect(stn, "MR") ~ max_depth$WS[3],
        .default = lat_in
    ),
    maximumDepthInMeters = case_when(
        str_detect(stn, "MR") ~ max_depth$MR[1],
        str_detect(stn, "LK") ~ max_depth$LK[1],
        str_detect(stn, "WS") ~ max_depth$WS[1]
    )
) %>%
  
  # === set up to align with next chunk var names
  rename(
    "taxa_orig"        = sci_name_guessed_by_tylar,
    "individualCount"  = ind_count,
    "lifeStage"        = lifestage,
    "decimalLatitude"  = lat_in,
    "decimalLongitude" = lon_in,
  ) %T>% 
  print()

# eventID   <- "jaime_protocol_observation"
datasetID <- "USF_IMaRS_MBON_compiled_zoo_taxonomy_jaimie_2018"

```

# 4.0 ---- Construct Occurence ID ----

```{r construct-occurrence-table}
# map col names into dwc
occur <-
  dat_merg_tx %>%
  dplyr::mutate(
    datasetID      = datasetID,
    parentEventID  = "IMaRS_MBON_zooplankton",
    datasetName    = paste(
      "MBON/USF-IMaRS/NOAA AOML Florida Keys National",
      "Marine Sanctuary Zooplankton Net Tows"
    ),
    collectionCode = "SEUS-MBON-Zooplankton",
    # make datetime and convert to utc
    eventDate    = lubridate::format_ISO8601(date_time, usetz = "Z"),
    eventID      = glue("{datasetID}:{sheet_nm}"),
    occurrenceID = glue("{eventID}:{clasification}"),
    occurrenceID = str_replace_all(occurrenceID, " ", "_"),
    occurrenceID = str_replace_all(occurrenceID, "\\.", ""),
    fieldNumber  = glue("{cruise_id}-{station}-{mesh}"),
    institutionCode = "USF | NOAA", # include NOAA
    
    type     = "Event",
    modified = format(Sys.Date(), "%Y-%m-%d"),
    language = "en",
    license  = "http://creativecommons.org/publicdomain/zero/1.0/legalcode",
    
    locationID   = stn,
    decimalLatitude,
    decimalLongitude,
    georeferenceVerificationStatus = "verified by contributor",
    higherGeography                = "North America | United States | Florida",
    continent                      = "North America",
    country                        = "United States",
    countryCode                    = "US",
    stateProvince                  = "Florida",
    geodeticDatum                  = "EPSG:4326",
    georeferencedBy                = "NOAA AOML | USF IMaRS | RSMAS R/V Walton Smith",
    recordedBy                     = "NOAA AOML | USF IMaRS",  
    higherGeographyID = case_when(
      str_detect(stn, "MR|LK|WS|9B") ~ "http://vocab.getty.edu/tgn/7030258",
      str_detect(stn, "5") ~ "http://vocab.getty.edu/tgn/1101513",
      .default = "Not Found"
    ),
    coordinateUncertaintyInMeters  = 500,
    minimumDepthInMeters           = 0,
    maximumDepthInMeters,
    habitat            = "near coral reef",
    

    # taxa names
    scientificName,
    scientificNameID, # like urn:lsid:ipni.org:names:37829-1:1.3
    identificationReferences = "WoRMS",
    verbatimIdentification   = clasification,    # organismQuantity         = ind_m3,
    taxonRank = rank,
    # dateIdentified           = date_analyzed,
    lifeStage,
    # counts of organisms?
    # IDk which of these is accurate
    # individualCount          = number_ind_sample,
    # add each aliquot as a total count per sample event
    individualCount,
    # organismQuantityType     = "Individuals per cubic metre",
    # or
    # measurementType          = "Number per cubic metre",
    # MOF? should also include ind/sample?
    # measurementUnitID        =
    # "http://vocab.nerc.ac.uk/collection/P06/current/UPMM/",
    # MOF?

    # TODO: check this information ----
    
    occurrenceStatus = "present",
    preparations     = "formalin before analysis | ethanol after analysis",
    basisOfRecord    = "PreservedSpecimen", # "humanObservation",
    disposition      = "in collection",
    identifiedBy     = "Jaimie Rojas-Marquez",
    identifiedByID   = "https://orcid.org/0009-0009-7318-7180",
    samplingProtocol = glue(
      "{mesh} mesh size (um) - ",
      "bongo nets | folson splitter | http://drs.nio.org/drs/handle/2264/95"
    ),
    net_type   = "bongo nets",
    microscopy = "microscopy",
    
    ind_m3               = total_ind_correct / volume_filt_cubic_m,
    ind_m3               = if_else(is.infinite(ind_m3), NA, ind_m3),
    organismQuantity     = ind_m3,
    organismQuantityType = "individuals per cubic metre",
    sampleSizeValue      = volume_filt_cubic_m,
    sampleSizeUnit       = "Volume per cubic metre of filtered seawater",
    samplingEffort       = str_c(tow_time_min, "minutes", sep = " "),
  ) %T>%
  print() 

```


```{r construct-occurrence-table}
filter(occur, is.na(scientificName)) %>%
 count(verbatimIdentification) 

occur <- filter(occur, !is.na(scientificName))
naniar::vis_miss(occur)
naniar::vis_miss(occur[,c(1:50)])
naniar::vis_miss(occur[,c(51:100)])
naniar::vis_miss(occur[,c(101:ncol(occur))])
# naniar::vis_miss(dat_merg_tx)
# naniar::vis_miss(dat_merg)
# Hmisc::describe(occur)
```
```{r}
na_rows <- occur[,c(101:ncol(occur))] %>%
  mutate(rows = row_number(), .before = 1) %>%
  
  filter(if_any(everything(), is.na)) %>%
  pull(rows)

occur[na_rows,]

# na_rows <- 
  occur[,c(51:100)] %>%
  mutate(rows = row_number(), .before = 1) %>%
    filter(not_used == "not used")
  janitor::remove_empty("cols")
  filter(if_any(everything(), is.na)) %>%
  pull(rows)
  
occur %>%
  mutate(
    .keep = "used",
    diff = scientific_name == scientificName
  ) %>%
    filter(diff == FALSE)

taxa_full %>%
  mutate(
    # .keep = "used",
    diff = scientific_name == scientificName
  ) %>%
    filter(diff == FALSE) %>%
  relocate(.after = taxa, scientific_name, scientificName)
```



```{r check-dupes}
janitor::get_dupes(occur, occurrenceID)
```

```{r save-occurrence-table}
# save locally
save_csv(
    .data          = occur,
    save_location  = here(base_dir, "processed"),
    save_name      = "occurrences",
    # overwrite      = TRUE, # <-- uncomment to overwrite
    verbose        = TRUE,
    time_stamp_fmt = "%Y%m%d_%H%M%S",
    utf_8          = TRUE
)

# if (FALSE) {
#   readr::write_excel_csv(
#     occur,
#     here::here(
#       base_dir, "processed",
#       glue(
#         "occurrences",
#         "{format(Sys.time(), '_%Y%m%d_%H%M%S')}.", "csv"
#       )
#     )
#   )
# }


# save to cloud if connection exists
if (FALSE && exists("cloud_dir") && !is.na(cloud_dir)) {
  master_taxa_list(
    taxa_list  = occur,
    .cloud_dir = here(cloud_dir, "old_jaime_data"),
    where_to   = "cloud",
    save       = TRUE,
    .file_expr = "occurrences",
    sheet_name = "Simplified Occurence Core"
  )
}
```

# 5.0 ---- Add More Cores ----
Quick first test.
TODO: update as needed

## 5.1 Record Level
```{r rcd-lvl}
taxa_matched_merg <-
  dat_merg_tx %>%
  mutate(
    type = "Event",
    modified = Sys.Date(),
    language = "en",
    license = "http://creativecommons.org/publicdomain/zero/1.0/legalcode",
    # institutionCode     = "USF_IMaRS",
    institutionCode = "USF",
    # parentEventID       = "IMaRS_MBON_zooplankton",
    datasetID = "USF_IMaRS_MBON_compiled_zoo_taxonomy_jaimie_2018",
    datasetName = paste(
      "MBON/USF-IMaRS Florida Keys National Marine Sanctuary",
      "Zooplankton Net Tows (2015 - 2017)"
    ),
    basisOfRecord = "PreservedSpecimen", # "HumanObservation",
    recordedBy = "NOAA AOML | USF IMaRS",
    identifiedBy   = "Jaimie Rojas-Marquez",
    identifiedByID = "https://orcid.org/0009-0009-7318-7180",
    # tow_time = case_when(
    #   str_detect(tow_time_min, ":") ~ str_extract(tow_time_min, "(\\d+):", 1),
    #   .default = tow_time_min
    # ),
    # tow_time = parse_number(tow_time),
    # tow_time = round(tow_time, 2)
    tow_time = round(tow_time_min, 2)
  )

rcd_lvl <- taxa_matched_merg %>%
    distinct()
```

## 5.2 Event Level Info
```{r event}
event <-
    taxa_matched_merg  %>%
    mutate(
        .keep = "none",
        # cruise_id, 
        sheet_nm,
        mesh, datasetID, type, modified, license, 
        institutionCode, datasetName, 
        language,
        locationID         = station,
        # eventDate          = lubridate::ymd_hms(
        #     stringr::str_c(date, local_time_est),
        #     tz = "EST"),
        # eventDate          = lubridate::format_ISO8601(eventDate, usetz = "Z"),
        eventDate          = lubridate::format_ISO8601(date_time, usetz = "Z"),
        eventDate          = if_else(is.na(eventDate), as.character(date), eventDate),
        eventID            = glue("{datasetID}:{sheet_nm}"),
        year               = year(date),
        month              = month(date),
        day                = day(date),
        samplingProtocol   = glue(
            "{mesh} mesh size (um) - ",
            "bongo nets | folson splitter | http://drs.nio.org/drs/handle/2264/95"
        ),
        
        # TODO: habitat NERC vocab
        habitat            = "near reef",
        # sampleSizeValue    = vol_filtered_m_3,
        sampleSizeValue    = volume_filt_cubic_m,
        sampleSizeUnit     = "Volume per cubic metre of filtered seawater",
        samplingEffort     = str_c(tow_time, "minutes", sep = " ")
    ) %>%
    distinct() %>%
    mutate(
        # TODO: save for the end when combining all same sizes together
        catalogNumber   = row_number(),
        .before = everything()
     ) %T>% 
  print()
```

## 5.3 Location Level Info:
```{r location}
location <-
    taxa_matched_merg %>%
    left_join(event, 
              by = join_by("sheet_nm", "station" == "locationID", "mesh")) %>%
    dplyr::mutate(
        .keep = "none",
        eventID,
        decimalLatitude, 
        decimalLongitude,
        higherGeographyID = case_when(
            str_detect(stn, "MR|LK|WS|9B") ~ "http://vocab.getty.edu/tgn/7030258",
            str_detect(stn, "5") ~ "http://vocab.getty.edu/tgn/1101513",
            .default = "Not Found"
        ),
        higherGeography   = "North America | United States | Florida",
        continent         = "North America",
        country           = "United States",
        countryCode       = "US",
        stateProvince     = "Florida",
        geodeticDatum     = "EPSG:4326",
        georeferencedBy   = "NOAA AOML | USF IMaRS | RSMAS R/V Walton Smith",
        coordinateUncertaintyInMeters = 500,
        minimumDepthInMeters = 0,
        maximumDepthInMeters
    ) %>%
    distinct()

left_join(
    event, location
) 
```

## 5.4 Ocurrence Level Info:
```{r occurence}
occur <-
  taxa_matched_merg %>%
  left_join(event,
    by = join_by(
      "sheet_nm",
      "station" == "locationID",
      "mesh"
    )
  ) %>%
  dplyr::mutate(
    .keep = "none",
    eventID,
    occurrenceID = glue("{eventID}:{clasification}"),
    occurrenceID = str_replace_all(occurrenceID, " ", "_"),

    # taxa names
    scientificName,
    across(kingdom:genus), # TODO: check if species column exists
    taxonRank = rank,
    organismQuantity = individualCount,
    organismQuantityType = "Summation of 1 mL Aliquots",
    lifeStage,
    occurrenceStatus = "present",
    preparations = "formalin before analysis | ethanol after analysis",
    scientificNameID,
    basisOfRecord,
    identificationReferences = "WoRMS",
    verbatimIdentification = clasification,
    georeferenceVerificationStatus = "verified by contributor",
    dispostion = "in collection"
  ) %>%
  relocate(eventID, occurrenceID,
    .before = 1
  ) %>%
  # removes row where couldn't get accurate aphia ID
  filter(!is.na(scientificNameID)) %T>%
  print()


```

# 5.5 Examples to save
```{r final-to-core}
event_final <- 
    left_join(event, location) %>%
    select(-sheet_nm, -datasetID, -catalogNumber, -mesh) 

# visualize missing    
naniar::vis_miss(event_final)
naniar::vis_miss(occur)

# how to merge event and occur
full_join(event_final, occur, by = "eventID") %>%
    naniar::vis_miss()

# check duplicates
stopifnot(nrow(janitor::get_dupes(event_final, eventID)) == 0)
stopifnot(nrow(janitor::get_dupes(occur, occurrenceID)) == 0) 
```


```{r save}
# save obis cores locally
pwalk(
  list(
    list(
     event_final,
     occur
     # eventually mof
    ),
    here(base_dir, "obis"),
    # glue("{c(\"event\", \"occur\", \"mof\")}_example_update")
    glue("{c(\"event\", \"occur\")}_example_update")
  ),
  \(data, loc, name_f)
    save_csv(
      .data          = data,
      save_location  = loc,
      save_name      = name_f,
      # overwrite      = FALSE,
      overwrite      = TRUE,
      verbose        = TRUE,
      time_stamp_fmt = "%Y-%m-%d",
      utf_8          = TRUE
    )
)

# old save
# if (FALSE) {
#   dir_sv <- here(base_dir, "obis")
#   path <-
#     here(
#       dir_sv,
#       glue(
#         "{c('event', 'occur', 'mof')}",
#         "_example_update_{Sys.Date()}.csv"
#       )
#     )
#   path
# 
#   # dir_create(dir_sv)
# 
#   event_final %>%
#     write_excel_csv(file = path[1], na = "")
# 
#   occur %>%
#     write_excel_csv(file = path[2], na = "")
# }

# save obis cores to cloud if connection exists
if (FALSE && exists("cloud_dir") && !is.na(cloud_dir)) {

    master_taxa_list(
    taxa_list  = event_final,
    .cloud_dir = here(cloud_dir, "old_jaime_data", "obis_cores"),
    where_to   = "cloud",
    save       = TRUE,
    .file_expr = "event_example",
    sheet_name = "Event Core"
  )   
    master_taxa_list(
    taxa_list  = occur,
    .cloud_dir = here(cloud_dir, "old_jaime_data", "obis_cores"),
    where_to   = "cloud",
    save       = TRUE,
    .file_expr = "occur_example",
    sheet_name = "Occurence Core"
  )   
}
```
