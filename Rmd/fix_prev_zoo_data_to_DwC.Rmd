---
title: "Fix and Convert Previous Zooplankton Data"
author: "Tylar Murray"
date: "2023-04-14"
format: html
---

# 1.0 ---- Setup ----
## 1.1 Load Libraries
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
if (!nzchar(system.file(package = "librarian"))) 
    install.packages("librarian")

librarian::shelf(
    librarian, ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr,
    forcats, lubridate, glue, fs, magrittr, here, conflicted,
    # broom # optional
    
    # additional
    cli, naniar, Hmisc
)

# shelf(conflicted) # may be needed if won't allow loading of certain packages

conflicts_prefer(
    dplyr::filter(), 
    dplyr::select()
    )
```

# 1.1 Make sure location of files is correct
From fix_prev_zoo_data.qmd
- create folders
  - place original data
  - place processed file to load here 
  
In case, you don't have these folder, creates them here
```{r create-folder}
base_dir <- here::here("data", "zoo_pre_2018")

fs::dir_create(here::here(base_dir, "processed"))
fs::dir_create(here::here(base_dir, "orig"))

```

## 1.2 Get File Paths
sp.csv from:
<https://drive.google.com/drive/folders/1IKBqpgOR6-bhJnm3wHwZAzvSYAA24Bsx>
put in "~/data/zoo_pre_2018"

```{r get-data-path}
# get file path to processed data
file_path <-
  here::here(base_dir, "processed") %>%
  fs::dir_ls(
    regexp  = "zoo_compiled_with_meta",
    recurse = TRUE
  ) %>%
  # function to check more recent made file to use
  last_mod() 

# read `.csv` file created by `fix_prev_zoo_data.Rmd`
dat <-
  file_path %>%
  # "../zoo_compiled_with_meta_20230411.csv" %>%
  
    readr::read_csv(
    file        = .,
    show_col_types = FALSE,
    guess_max   = 10000,
    name_repair = janitor::make_clean_names
  )

species_path <-
    here::here(base_dir) %>%
    fs::dir_ls(regexp = "sp\\.csv")

# read manually created verbatim name map `sp.csv`
species_map <-
  species_path %>%
  # "../sp.csv" %>%
  
  readr::read_csv(
    file = .,
    show_col_types = FALSE,
    name_repair    = janitor::make_clean_names,
    na = c("", "NA", stringi::stri_dup("?", 1:5)) # ignore `?` 1 to 5 times
  )
```

# 2.0 ---- Merge Taxa Info ----
## 2.1 Merge Species Info with Raw Data
```{r load-csv-data}
# map Jaime's species names to WoRMS-readable using `sp.csv`
dat_merg <-
  # dplyr::select(species_map, 1:2, 4) %>% # select 1st 2 columns
  dplyr::select(species_map,
                jaimes_taxa_info,
                contains("sci_name"),
                lifestage) %>% # select 1st 2 columns

  # join on classification columns. `.` is where it gets piped into
  dplyr::left_join(
    dat,
    .,
    by = c("clasification" = "jaimes_taxa_info")
  ) %>%
  # construct unique identifier for each row (used for occurrenceID later)
  dplyr::mutate(
    orig_data_row_ID = glue("{sheet_nm}:{clasification}:{lifestage}"),
  ) %>%
    distinct()
```

## 2.2 Check Duplicates for Unique IDs
```{r check-duplicates}
# validate: rows in file must be unique
# janitor::get_dupes(<dataframe name>, <column names to group together>) 

dupes <- janitor::get_dupes(dat_merg, orig_data_row_ID)

# Check if duplicated rows
if (nrow(dupes) > 0) {
    stop(
        glue("There are duplicated rows in merged data! ",
             "(n = {nrow(dupes)})")
        )
} else {
    message("No duplicated `orig_data_row_ID`.")
}

# only flowmeter data differs and one looks unrealistically large.
```

# 3.0 ---- Get AphiaID from WoRMS ----
Using `merge_taxa()` 
check = if want to check unmatched names
viewer = if want to see unmatched taxa using View()
```{r aphia-id}
# set location for aphiaID list and the base of the file name
file_exprs <- 
    file_expr(
    # loc       = here::here("data", "metadata", "aphia_id"), 
    loc       = here::here(base_dir), 
    file_base = "aphia_taxa_jaime"
    )

max_depth <-
  list(
    MR = 36.5,
    LK = 40.5,
    WS = 23
  )

taxa <-
    dat_merg %>%
    select("sci_name_guessed_by_tylar") %>%
    distinct() %>%
    merge_taxa(., 
               .file_expr = file_exprs, 
               check      = FALSE, 
               use_cloud = FALSE
               # viewer     = TRUE, 
               ) %>%
    mutate(
        aphiaID = if_else(!is.na(scientificNameID), 
                          str_split(scientificNameID, ":", 
                                    simplify = TRUE)[, 5],
                          NA_character_) %>%
                          as.numeric(.),
        info = map(aphiaID, 
                   ~ tryCatch({
                       cat("\n", .x,)
                       worrms::wm_record(.x)
                       }, error = function(e) {
                           return(NULL)
                           }))
    ) %>%
    unnest(info, names_repair = janitor::make_clean_names) %>%
    select(-lifestage) %>% 
    rename("scientificName" = scientific_name,
           "scientificNameID" = scientific_name_id)
```


```{r aphia-id}
dat_merg_tx <- 
    # select(taxa, 1, 4, 5)  %>%  # select 1st 2 columns
    # select(taxa, 1, 4, 5)  %>%  # select 1st 2 columns
  left_join(
      dat_merg,
      taxa,
      by = c("sci_name_guessed_by_tylar" = "taxa_orig")
  )  # join on classification columns. `.` is where it gets piped into

dat_merg_tx %>%
    select(contains("lifest")) %>%
    filter(!is.na(lifestage))

# round individual count. we don't know why there are fractional counts. 
dat_merg_tx <- 
    dat_merg_tx %>%
    
    mutate(
    
    ind_count = round(ind_count),
    # hardcode missing lat lon using averages in water-samples
    lon_in = case_when(
        is.na(lon_in) & str_detect(stn, "LK") ~ -81.41464542,
        is.na(lon_in) & str_detect(stn, "WS") ~ -81.71532752,
        is.na(lon_in) & str_detect(stn, "MR") ~ -80.37953386,
        TRUE ~ lon_in
    ),
    lat_in = case_when(
        is.na(lat_in) & str_detect(stn, "LK") ~ 24.53862793,
        is.na(lat_in) & str_detect(stn, "WS") ~ 24.47605506,
        is.na(lat_in) & str_detect(stn, "MR") ~ 25.00607727,
        TRUE ~ lat_in
    ),
    maximumDepthInMeters = case_when(
        str_detect(stn, "MR") ~ max_depth$MR,
        str_detect(stn, "LK") ~ max_depth$LK,
        str_detect(stn, "WS") ~ max_depth$WS
    )
) %>%
  
  # === set up to align with next chunk var names
  rename(
    
    "taxa_orig"        = sci_name_guessed_by_tylar,
    "individualCount"  = ind_count,
    "lifeStage"        = lifestage,
    "decimalLatitude"  = lat_in,
    "decimalLongitude" = lon_in,
  )

# eventID   <- "jaime_protocol_observation"
datasetID <- "USF_IMaRS_MBON_compiled_zoo_taxonomy_jaimie_2018"

```

# 4.0 ---- Construct Occurence ID ----
```{r construct-occurrence-table}
names(dat2)

# map col names into dwc
occur <-
    dat_merg_tx %>%
    # left_join(event, 
    #           by = c("cruise_id", 
    #                  "site" = "locationID", 
    #                  "mesh" #, 
    #                  # "recordedBy", 
    #                  #"recordedByID", "basisOfRecord"
    #                  )
    #           ) %>%
    # dplyr::transmute(
    # this method is prefered to maintainers instead of transmute
    dplyr::mutate(
    .keep = "none",
        # ---
        
        decimalLatitude,
        decimalLongitude,
        # eventID, 
        # eventDate                = glue("{date}T{local_time_est}ET"),
        # make datetime and convert to utc 
        eventDate = lubridate::ymd_hms(stringr::str_c(date, local_time_est),  
                                       tz = "EST"),
        eventDate = lubridate::format_ISO8601(eventDate, usetz = "Z"),
        eventDate = if_else(is.na(eventDate), as.character(date), eventDate),
    
        # occurrenceID             = glue("{eventID}:{aphiaID}:{lifeStage}"),
        # changed to taxa_orig to make sure occur ID is unique
        # occurrenceID             = glue("{eventDate}:{eventID}:{taxa_orig}:{lifeStage}"),
        occurrenceID             = glue("{datasetID}:{sheet_nm}:{clasification}"),
        occurrenceID = str_replace_all(occurrenceID, " ", "_"),
        # taxa names
        scientificName,
        # across(kingdom:genus),
        # taxonRank = rank,
        # taxonID = taxon_rank_id,
         
        # recorded by
        # recordedBy,
        # recordedByID,
        # dateIdentified           = date_analyzed,
        
        # counts of organisms?
        # IDk which of these is accurate
        # individualCount          = number_ind_sample,
        # add each aliquot as a total count per sample event
        individualCount,
        # organismQuantity         = ind_m3,
        # organismQuantityType     = "Individuals per cubic metre",
        # or
        # measurementType          = "Number per cubic metre",
        # MOF? should also include ind/sample?
        # measurementUnitID        = 
        # "http://vocab.nerc.ac.uk/collection/P06/current/UPMM/",
        # MOF?
        
        # TODO: check this information ----
        lifeStage,
        # establishmentMeans       = "native | uncertain",
        occurrenceStatus         = "present",
        preparations             = "formalin before analysis | ethanol after analysis",
        scientificNameID,
        # like urn:lsid:ipni.org:names:37829-1:1.3
        basisOfRecord = "PreservedSpecimen", #"humanObservation",
        datasetID = {datasetID},
        identificationReferences = "WoRMS",
        verbatimIdentification   = clasification,
        # georeferenceVerificationStatusProperty = "verified by contributor",
        georeferenceVerificationStatus = "verified by contributor",
        dispostion = "in collection",
        coordinateUncertaintyInMeters = 500,
        minimumDepthInMeters = 0,
        maximumDepthInMeters,
        # identifiedBy = "Jamie"
    )



occur <- filter(occur, !is.na(scientificNameID))
naniar::vis_miss(occur)
# naniar::vis_miss(dat_merg_tx)
# naniar::vis_miss(dat_merg)
Hmisc::describe(occur)
```


```{r}
get_dupes(occur, occurrenceID)
```

```{r construct-occurrence-table}
if (FALSE)
    readr::write_csv(
        occur, 
        here::here(base_dir, "processed", 
        glue("occurrences", 
    "{format(Sys.time(), '_%Y%m%d_%H%M%S')}.", "csv"))
)
```

# 5.0 ---- Add More Cores ----
Quick first test.
TODO: update as needed

## 5.1 Record Level
```{r rcd-lvl}
taxa_matched_merg <-
  dat_merg_tx %>%
  mutate(
    type = "Event",
    modified = Sys.Date(),
    language = "en",
    license = "http://creativecommons.org/publicdomain/zero/1.0/legalcode",
    # institutionCode     = "USF_IMaRS",
    institutionCode = "USF",
    # parentEventID       = "IMaRS_MBON_zooplankton",
    datasetID = "USF_IMaRS_MBON_compiled_zoo_taxonomy_jaimie_2018",
    datasetName = paste(
      "MBON/USF-IMaRS Florida Keys National Marine Sanctuary",
      "Zooplankton Net Tows (2015 - 2017)"
    ),
    basisOfRecord = "PreservedSpecimen", # "HumanObservation",
    # informationWithheld = "collector identities withheld because changed frequently",
    # recordedBy          = nlf,
    recordedBy = "NOAA AOML",
    # recordedByID        = orcid_nat,
    # Abbey B. recommends this change, updated March 21, 2023
    identifiedBy = "Jamie <TODO>",
    tow_time = case_when(
      str_detect(tow_time_min, ":") ~ str_extract(tow_time_min, "(\\d+):", 1),
      .default = tow_time_min
    ),
    tow_time = parse_number(tow_time),
    tow_time = round(tow_time, 2)
  )

rcd_lvl <- taxa_matched_merg %>%
    distinct()
```

## 5.2 Event Level Info
```{r event}
event <-
    taxa_matched_merg  %>%
    mutate(
        .keep = "none",
        # cruise_id, 
        sheet_nm,
        mesh, datasetID, type, modified, license, 
        institutionCode, datasetName, 
        language,
        locationID         = station,
        eventDate          = lubridate::ymd_hms(
            stringr::str_c(date, local_time_est),
            tz = "EST"),
        eventDate          = lubridate::format_ISO8601(eventDate, usetz = "Z"),
        eventDate          = if_else(is.na(eventDate), as.character(date), eventDate),
        eventID            = glue("{datasetID}:{sheet_nm}"),
        year               = year(date),
        month              = month(date),
        day                = day(date),
        samplingProtocol   = glue(
            "{mesh} mesh size (um) - ",
            "bongo nets | folson splitter | http://drs.nio.org/drs/handle/2264/95"
        ),
        
        # TODO: habitat NERC vocab
        habitat            = "near reef",
        sampleSizeValue    = vol_filtered_m_3,
        sampleSizeUnit     = "Volume per cubic metre of filtered seawater",
        samplingEffort     = str_c(tow_time, "minutes", sep = " ")
    ) %>%
    distinct() %>%
    mutate(
        # TODO: save for the end when combining all same sizes together
        catalogNumber   = row_number(),
        .before = everything()
     ) 
```

## 5.3 Location Level Info:
```{r location}
location <-
    taxa_matched_merg %>%
    left_join(event, 
              by = join_by("sheet_nm", "station" == "locationID", "mesh")) %>%
    dplyr::mutate(
        .keep = "none",
        eventID,
        decimalLatitude, 
        decimalLongitude,
        higherGeographyID = case_when(
            str_detect(stn, "MR|LK|WS|9B") ~ "http://vocab.getty.edu/tgn/7030258",
            str_detect(stn, "5") ~ "http://vocab.getty.edu/tgn/1101513",
            TRUE ~ "Not Found"
        ),
        higherGeography   = "North America | United States | Florida",
        continent         = "North America",
        country           = "United States",
        countryCode       = "US",
        stateProvince     = "Florida",
        geodeticDatum     = "EPSG:4326",
        georeferencedBy   = "NOAA AOML | USF IMaRS | RSMAS R/V Walton Smith",
        coordinateUncertaintyInMeters = 500,
        minimumDepthInMeters = 0,
        maximumDepthInMeters
    ) %>%
    distinct()

left_join(
    event, location
) 
```

## 5.4 Ocurrence Level Info:
```{r occurence}
occur <-
  taxa_matched_merg %>%
  left_join(event,
    by = join_by(
      "sheet_nm",
      "station" == "locationID",
      "mesh"
    )
  ) %>%
    dplyr::mutate(
        .keep = "none",
        eventID, 
        occurrenceID             = glue("{eventID}:{clasification}"),
        
        # taxa names
        scientificName,
        across(kingdom:genus), # TODO: check if species column exists
        taxonRank = rank,
        organismQuantity         = individualCount,
        organismQuantityType     = "Summation of 1 mL Aliquots",
        

        lifeStage,
        occurrenceStatus         = "present",
        preparations             = "formalin before analysis | ethanol after analysis",
        scientificNameID,
        basisOfRecord,
        identificationReferences = "WoRMS",
        verbatimIdentification   = clasification,
        georeferenceVerificationStatus = "verified by contributor",
        dispostion = "in collection"
    ) %>%
      relocate(eventID, occurrenceID, 
               .before = 1) %>%
    
    # removes row where couldn't get accurate aphia ID
    filter(!is.na(scientificNameID))


```

# 5.5 Examples to save
```{r final-to-core}
event_final <- 
    left_join(event, location) %>%
    select(-sheet_nm, -datasetID, -catalogNumber, -mesh) 

# visualize missing    
naniar::vis_miss(event_final)
naniar::vis_miss(occur)

# how to merge event and occur
full_join(event_final, occur, by = "eventID") %>%
    naniar::vis_miss()

# check duplicates
stopifnot(nrow(get_dupes(event_final, eventID)) == 0)
stopifnot(nrow(get_dupes(occur, occurrenceID)) == 0) 
```


```{r save}
if (FALSE) {
  dir_sv <- here(base_dir, "obis")
  path <-
    here(
      dir_sv,
      glue(
        "{c('event', 'occur', 'mof')}",
        "_example_update_{Sys.Date()}.csv"
      )
    )
  path

  dir_create(dir_sv)

  event_final %>%
    write_csv(file = path[1], na = "")

  occur %>%
    write_csv(file = path[2], na = "")
}
```