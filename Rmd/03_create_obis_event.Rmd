---
title: "OBIS Event Creations"
author: "Sebastian DiGeronimo"
date: '2022-09-02'
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---
TODO: check how to save UTF-8 encoding on csv files 
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
librarian::shelf(
    librarian, ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr,
    forcats, lubridate, glue, fs, magrittr, here,
    # broom # optional
    
    # additional
    
)

library("conflicted")

conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")

library("readxl")
library("hablar") # might be useful when needing to fix names
library("worrms")
library("geosphere")


# obistools::event_fields()
# source(here::here("scripts","log_file_changes.R"))
# startup("log_edits")
# log_add()

# ---- recorded by info
nlf       <- "Natalia Lopez Figueroa"
orcid_nat <- "https://orcid.org/0000-0002-7527-0481"

# ---- set variables for calculation
inpeller <- 26873 / 999999 # starting oct 2022 for 200/500 um
# net_area <- pi * 0.5^2

# TODO: check this equation ---------------------------------------------------
which_one <- "nat"
equation_zoo <- switch(
    which_one,
    "nat"    = expression(dillution_factor * mean * 2 ^ (splits_analyzed)),
    "jamie"  = expression(dillution * mean * splits_analyzed / pipette_vol_m_l),
    "jamie2" = expression(dillution * mean)
)
rm(which_one)
```

# Load functions if not already
This will load the functions needed if not already contain in the search path.
```{r load-functions}
source(here("scripts", "attach_funcs.R"))
func_attach()
rm(func_attach)
```

Workflow:
<https://github.com/ioos/bio_data_guide/tree/main/OBIS_data_tiers>
Needed: 3 things 1. Event 2. Occurances 3. Measurement of Fact (MOF)

1.  Read and format metadata spreadsheets
    -   convert to DarwinCore format

MOF = measurement of fact

Event - where/when took place

Occurrence - species info, presence/absence Occurrence MOF - quantity,
other info to measure them

MOF - environmental quantities measured

# not finished yet

occurrence = institution_code = "USF_IMaRS" 
collection_code = "compiled_zoo_taxonomy_nlf" 
catalog_number = "20yy_mm_dd" 
occurrenceID = (unique identifier) 
joining of <urn:catalog> institution code
collection_code catalog_number row number 
join by : 
eventDate = date_time 
decimalLongitude = lon_in 
decimalLatitude = lat_in
scientificName = natalia spreadsheet 
scientificNameID = "TODO: worms lookup", #taxonomy_df["classification"],

occurence = basisOfRecord = "HumanObservation", 
collectionCode = collection_code, 
catalogNumber = catalog_number, 
occurrenceStatus = "present", 
institutionCode = institution_code

extra-stuff mesh_size flowmeter in flowmeter out ship speed inpeller
constant distnace tow speed formalin vol filtered

# Load data for OBIS conversion
```{r load-data}
# ---- load aphia id
aphia_id <-
    fs::dir_ls(path = here::here("data", "metadata", "aphia_id"),
               regexp = "^[^~]*(aphia)+.*\\.csv$") %>%
    last_mod(.) %>%
    read_csv(show_col_types = FALSE) %>%
    mutate(
        aphiaID = if_else(!is.na(scientificNameID), 
                          str_split(scientificNameID, ":", 
                                    simplify = TRUE)[, 5],
                          NA_character_) %>%
                          as.numeric(.),
        info = map(aphiaID, 
                   ~ tryCatch({worrms::wm_record(.x)
                       }, error = function(e) {
                           return(NULL)
                           }))
    ) %>%
    unnest(info, names_repair = janitor::make_clean_names) %>%
    select(!contains("_2"), -c(2:6))

# ---- load metadata
meta_df <-
    fs::dir_ls(path = here::here("data", "metadata", "cruise_logsheets"),
               regexp = "^[^~]*(meta_)+.*\\.csv$") %>%
    last_mod(.) %>%
    read_csv(show_col_types = FALSE) %>%
    
    # this is when Natalia LF started analysis
    filter(date >= date("2017-06-18")) %>%
    
    mutate(locationID      = case_when(
                str_detect(station, "Mol")  ~ "MR",
                str_detect(station, "Loo")  ~ "LK",
                str_detect(station, "West") ~ "WS",
                str_detect(station, "9B")   ~ "9B",
                TRUE ~ station), 
           .after = station) %>%
    mutate(
        # Note: flowmeter is MF315
        net_size            = 0.5,
        net_area            = pi * 0.5^2,
        distance_m_2        = (flowmeter_out - flowmeter_in) * inpeller_constant,
        tow_speed_m_sec     = distance_m_2 / (tow_time_min * 60),
        # tow_speed_cm_sec     = distance_m_2 * 100 / (tow_time_min * 60),
        volume_filt_cubic_m = net_area/4 * distance_m_2,
        split_size          = map(split_size, function(x) {
                                    out <- tryCatch({
                                    eval(parse(text = x))}, 
                                    error = function(e) {NA_integer_})}) %>% 
                              unlist(.)
       ) 

# ---- load zooplankton data
taxa_files <-
    fs::dir_ls(path = here::here("data", "processed"),
               regexp = "all_merged_processed") %>%
    last_mod(.) %>%
    map_dfr(~ read_csv(., show_col_types = FALSE)) %>%
    mutate(
        # fix some cruise ID from data spreadsheets
        # cruise_id = case_when(
        #     # str_detect(cruise_id, "") ~ "", # incase other become problematic
        #     TRUE ~ cruise_id
        #     ),
        site      = str_replace(site, "MK", "MR"),
        lifeStage = case_when(
            is.na(lifeStage) ~ "adult",
            TRUE ~ lifeStage)
    ) %>%
  select(-mean_ind_dil_factor) %>%
  rowwise(aliquot_1:aliquot_3) %>%
  mutate(
      individualCount = sum(aliquot_1, 
                            aliquot_2, 
                            aliquot_3, 
                            na.rm = TRUE),
      .before = aliquot_1
  ) %>%
  ungroup()

# ---- merge metadata and taxa data
taxa_matched_merg <-
    taxa_files %>%
    left_join(meta_df, 
              by = c("cruise_id", 
                     "site" = "locationID", 
                     "mesh" = "mesh_size_um")) %>%
    mutate(
        # split_amount = 0.5, # splits from cruise
        splits_analyzed   = case_when(
            is.na(splits_analyzed) ~ 0,
            TRUE ~ splits_analyzed
            ),
        number_ind_sample = eval(equation_zoo),
        ind_m3            = number_ind_sample / volume_filt_cubic_m,
        
        # get aphia ID from end of scientificNameID
        aphiaID           = if_else(!is.na(scientificNameID),
                                    str_split(scientificNameID, ":", 
                                              simplify = T)[, 5],
                                    NA_character_) %>%
                          as.numeric(.)
        ) %>%
    left_join(., 
              aphia_id,
              by = c("taxa_orig" = "taxa_orig", 
                     "aphiaID"   = "aphia_id")) 
```

# Ex: Occurrence Bare Minimum example
set to `TRUE` if want to run
```{r obis-file}
# if (FALSE) {
#    data.table::data.table(
#           occurrenceID = c("my-dataset-0001c29"),
#        decimalLatitude = c(-87.7575),
#       decimalLongitude = c(24.4727),
#         scientificName = c("Sparisoma aurofrenatum"),
#       scientificNameID = c("urn:lsid:ipni.org:names:37829-1:1.3"),
#       occurrenceStatus = c("present"),
#          basisOfRecord = c("HumanObservation"),
#              datasetID = c("my-dataset-tylar-2020-01-08-123456"),
#              eventDate = c("2010-01-03T13:44Z")
#     )
# }
```

# Record Level Info
```{r record-level}
taxa_original <- taxa_matched_merg
taxa_matched_merg <-
    taxa_original  %>%
    mutate(
        type                = "Event",
        modified            = Sys.Date(),
        language            = "en",
        license             = "http://creativecommons.org/publicdomain/zero/1.0/legalcode",
        # institutionCode     = "USF_IMaRS",
        institutionCode     = "USF",
        parentEventID       = "IMaRS_MBON_zooplankton",
        # datasetName         = "MBON Florida Keys National Marine Sanctuary",
        datasetName         = glue("MBON Florida Keys National Marine Sanctuary", 
                                   "Zooplankton Net Tows (",
                                   "{min(year(date_time), na.rm = TRUE)} - ",
                                   "{max(year(date_time), na.rm = TRUE)})"),
        basisOfRecord       = "PreservedSpecimen", # "HumanObservation",
        # informationWithheld = "collector identities withheld because changed frequently",
        recordedBy          = nlf,
        recordedByID        = orcid_nat,
    )

rcd_lvl <- taxa_matched_merg %>%
    distinct()

rm(nlf, orcid_nat)
```

# Event Level Info
```{r event}
event <-
    taxa_matched_merg  %>%
    transmute(
        cruise_id, mesh, parentEventID, type, modified, license, 
        institutionCode, datasetName, 
        # basisOfRecord,  informationWithheld,
        #recordedBy, recordedByID, 
        language,
        # catalogNumber   = row_number(),
        locationID = site,
        # parentEventID      = "IMaRS_MBON_zooplankton",
        # datasetID          = glue("{parentEventID}:{cruise_id}"),
        # datasetID          = glue("{cruise_id}"),
        # eventDateTime      = format(date_time, "%Y-%m-%dT%H:%m:%S%z"),
        eventDate          = format_ISO8601(date_time, usetz = "Z"),
        # eventID - something like cruiseID:stationID:meshsize
        # {{mbon} : cruise} : stn : mesh : date_time
        eventID            = glue(
                        # "{datasetID}:stn{locationID}:{mesh}um:{eventDateTime}"
                        "{cruise_id}:stn{locationID}:{mesh}um:{eventDate}"
        )  %>% str_remove("\\+0000"),
        fieldNumber        = glue("{cruise_id}-{station}-{mesh}"),
        # eventDate          = date(date_time), 
        # eventTime          = format(date_time, "%H:%m:%S%z"),
        year               = year(date_time),
        month              = month(date_time),
        day                = day(date_time),
        # samplingProtocol = paste(mesh_size_um, "mesh size (um)"),
        samplingProtocol   = glue(
            "{mesh} mesh size (um) - ",
            "bongo nets | http://drs.nio.org/drs/handle/2264/95"
        ),
        
        # TODO: habitat NERC vocab
        habitat            = "near reef",
        sampleSizeValue    = volume_filt_cubic_m,
        sampleSizeUnit     = "Volume per cubic metre of filtered seawater",
        samplingEffort     = str_c(tow_time_min, "minutes", sep = " ")
    ) %>%
    distinct() %>%
    mutate(
        # TODO: save for the end when combining all same sizes together
        catalogNumber   = row_number(),
        .before = everything()
     ) 
```

# Location Level Info:
```{r location}
location <-
    taxa_matched_merg %>%
    left_join(event, by = c("cruise_id", "site" = "locationID", "mesh")) %>%
    dplyr::transmute(
        eventID,
        decimalLatitude   = lat_in,
        decimalLongitude  = lon_in,
        higherGeographyID = case_when(
            str_detect(site, "MR|LK|WS|9B") ~ "http://vocab.getty.edu/tgn/7030258",
            str_detect(site, "5") ~ "http://vocab.getty.edu/tgn/1101513",
            TRUE ~ "Not Found"
        ),
        higherGeography   = "North America | United States | Florida",
        continent         = "North America",
        country           = "United States",
        countryCode       = "US",
        stateProvince     = "Florida",
        geodeticDatum     = "EPSG:4326",
        georeferencedBy   = "NOAA AOML | USF IMaRS | RSMAS R/V Walton Smith"
    ) %>%
    distinct()

left_join(
    event, location
)

```

# Ocurrence Level Info:
```{r occurence}
occur <-
    taxa_matched_merg %>%
    left_join(event, 
              by = c("cruise_id", 
                     "site" = "locationID", 
                     "mesh" #, 
                     # "recordedBy", 
                     #"recordedByID", "basisOfRecord"
                     )
              ) %>%
    dplyr::transmute(
        eventID, 
        # occurrenceID             = glue("{eventID}:{aphiaID}:{lifeStage}"),
        # changed to taxa_orig to make sure occur ID is unique
        occurrenceID             = glue("{eventID}:{taxa_orig}:{lifeStage}"),
        
        # taxa names
        scientificName,
        across(kingdom:genus),
        taxonRank = rank,
        # taxonID = taxon_rank_id,
         
        # recorded by
        recordedBy,
        recordedByID,
        dateIdentified           = date_analyzed,
        
        # counts of organisms?
        # IDk which of these is accurate
        # individualCount          = number_ind_sample,
        # add each aliquot as a total count per sample event
        individualCount,
        organismQuantity         = ind_m3,
        organismQuantityType     = "Individuals per cubic metre",
        # or
        # measurementType          = "Number per cubic metre",
        # MOF? should also include ind/sample?
        # measurementUnitID        = 
        # "http://vocab.nerc.ac.uk/collection/P06/current/UPMM/",
        # MOF?
        
        # TODO: check this information ----
        lifeStage,
        # establishmentMeans       = "native | uncertain",
        occurrenceStatus         = "present",
        preparations             = "formalin before analysis | ethanol after analysis",
        scientificNameID,
        # like urn:lsid:ipni.org:names:37829-1:1.3
        basisOfRecord,
        # datasetID,
        # eventDate                = date(date_time),
        identificationReferences = "WoRMS",
        verbatimIdentification   = taxa_orig,
        # georeferenceVerificationStatusProperty = "verified by contributor",
        georeferenceVerificationStatus = "verified by contributor",
        dispostion = "in collection"
    )


```
# MoF
## Ex: Mearuement or Fact
MoF https://github.com/ioos/bio_data_guide/blob/main/datasets/WBTS_MBON/IOOS%20DMAC%20DataToDwC_Notebook_event.ipynb
```{r MoF-example}
# # 
# if (FALSE) {
# # ---- Measurement or Fact
# 
# tibble::tribble(
#               ~Origin.Term,                                       ~measurementTypeID,       ~URI,
#                 "Net_Type",                                             "plankton net",       "22",
#                "Mesh_Size",                                   "Sampling net mesh size", "Q0100015",
#                "NET_DEPTH",       "Depth (spatial coordinate) of sampling event start", "DXPHPRST",
#                  "COMMENT",            "N/A (mapped to measurementRemark field above)",      "N/A",
#        "Plankton_Net_Area",                    "Sampling device aperture surface area", "Q0100017",
#          "Volume_Filtered",                                                   "Volume",      "VOL",
#             "Sample_Split", "N/A (information added to measurementRemark field above)",      "N/A",
#        # "Sample_Dry_Weight",                                       "Dry weight biomass", "ODRYBM01",
#                 # "DW_G_M_2",                                       "Dry weight biomass", "ODRYBM01",
#          "Dilution_Factor",                                                      "???",      "???",
#     "TOTAL_DILFACTOR_CFIN",                                                      "???",      "???"
#     )
# 
# 
# 
# # ---- example MoF
# 
# # column_mappings <- 
#     tibble::tribble(
# ~orig_term,                             ~uri,     ~unit,             ~unitID, ~accuracy,      ~measurementTypeID,         ~measurementMethod,
# "Net_Type",                "L05/current/22/",        NA,                  NA,   NA,                   "net type",                         NA,
# "Bongo Nets",        "L22/current/NETT0176/",        NA,                  NA,   NA,                           NA,                         NA, 
# "Mesh_Size",         "Q01/current/Q0100015/", "microns", "P06/current/UMIC/",   NA,                  "mesh size",                         NA,
# "NET_DEPTH",         "P01/current/DXPHPRST/",       "m", "P06/current/UPAA/",   NA,                  "net depth",                         NA,
# "Plankton_Net_Area", "Q01/current/Q0100017/",      "m2", "P06/current/UPAA/",   NA,          "plankton net area",                         NA,
# "Volume_Filtered",        "P25/current/VOL/",      "m3", "P06/current/UPAA/",   NA,            "volume filtered", "geometrically determined",
# "Dilution_Factor",                        NA,      "ml", "P06/current/VVML/",   NA,            "dilution factor",                         NA,
# "TOTAL_DILFACTOR_CFIN",                   NA,      "ml", "P06/current/VVML/",   NA, "Total Dilution factor CFIN",                         NA,
# "Sample_Split",                           NA, "decimal", "P06/current/UPCT/",   NA,               "sample split",          "Folsom Splitter"
#         )
# }
```
## Meaurement or Fact Level Info
URL for controlled vocab: "http://vocab.nerc.ac.uk/collection/"
```{r MoF-base}
# TODO: edit MoF for more information
# result of script is MoF1 variable
source("../scripts/mof_base_create_table.R")
MoF1 <- mof_read()
# need:
# measurementType, measurementTypeID, measurementValue, measurementUnit, measurementAccuracy, measurementDeterminedBy, measurementMethod, measurementRemarks
def_web <- "http://vocab.nerc.ac.uk/collection/"

MoF1 <-
    MoF1 %>%
    mutate(
        measurementTypeID = case_when(
            !is.na(measurementType_uri) & str_length(measurementType_uri) > 1 ~
                str_c(def_web, measurementType_uri),
            TRUE ~ ""
        ),
        measurementUnitID = case_when(
            !is.na(measurementUnit_uri) & str_length(measurementUnit_uri) > 0 ~
                str_c(def_web, measurementUnit_uri),
            TRUE ~ ""
        )
    )  %>%
    select(-measurementValue & !ends_with("_uri"))

MoF1
```

```{r MoF}
# # ---- Event MoF
MoF_event  <-
    taxa_matched_merg %>%
     left_join(event, 
              by = c("cruise_id", 
                     "site" = "locationID", 
                     "mesh"#, 
                     # "recordedBy", 
                     # "recordedByID", 
                     # "basisOfRecord"
                     )
              ) %>%
    distinct(eventID, .keep_all = TRUE) %>%
    select(-c(aphiaID:recordedByID)) %>%
    mutate(
        Net_type = "bongo nets",
        microscopy = "microscopy"
    ) %>%
    
    pivot_longer(
                 cols = MoF1$orig_term[-1],
        # cols = c("split_amount", "mesh", "ind_m3"),
                 names_to = "orig_term",
                 values_to = "measurementValue",
        values_transform = list(measurementValue = as.character)
    ) %>%
    select(# datasetID, 
           eventID, orig_term, measurementValue) %>%
    left_join(MoF1, by = c("orig_term")) %>%
    filter(event_occur == "event")

# ---- Occurence MoF
MoF_occur  <-
    occur %>%
        select(eventID, occurrenceID, 
               "number_ind_sample" = individualCount, 
               "ind_m3" = organismQuantity,
               verbatimIdentification, scientificName) %>%
        
        pivot_longer(
                     cols = c(number_ind_sample, ind_m3),
            # cols = c("split_amount", "mesh", "ind_m3"),
                     names_to = "orig_term",
                     values_to = "measurementValue",
            values_transform = list(measurementValue = as.character)
        ) %>%
        select(occurrenceID, orig_term, measurementValue) %>%
        left_join(MoF1, by = c("orig_term")) 
        filter(event_occur == "event")

MoF <- 
    bind_rows(MoF_event, MoF_occur) %>%
    relocate(occurrenceID, 1) %>%
    select(-orig_term, -event_occur)

```

# Examples to save
```{r save}
set.seed(1234)
dir <- here("data", "processed", "obis_example")
path <-
    here(dir, 
         glue("{c('event', 'occur', 'mof')}",
              "_example_update_{Sys.Date()}.csv"))
path

dir_create(dir)

events <- sample(unique(event$eventID), 5)

left_join(event, location) %>%
    select(-cruise_id, -parentEventID, -catalogNumber, -mesh) %>%
    # group_by(eventID) %>%
    filter(eventID %in% events) %>% 
    # filter(eventID == "IMaRS_MBON_zooplankton:SV18067:stnLK:500um:2018-03-09T15:03:00") %>%
    write_csv(file = path[1], na = "")

occur %>%
    # group_by(eventID) %>%
    filter(eventID %in% events) %>%
    # filter(eventID == "IMaRS_MBON_zooplankton:SV18067:stnLK:500um:2018-03-09T15:03:00") %>%
    write_csv(file = path[2], na = "")

MoF %>%
    # group_by(eventID) %>%
    filter(eventID %in% events | str_detect(occurrenceID, paste(events, collapse = "|"))) %>%
    # filter(eventID == "IMaRS_MBON_zooplankton:SV18067:stnLK:500um:2018-03-09T15:03:00") %>%
    write_csv(file = path[3], na = "")

```

```{r}
reduce(list(event, location, occur, MoF), left_join) %>%
    obistools::check_depth(.)

taxa_matched_merg %>%
    select(scientificName, aphiaID) %>%
    slice_head(n = 10) %>%
    transmute(
        scientificName,
    info = map(aphiaID, ~ worrms::wm_record(.x))
) %>%
  unnest(info)
reduce(list(event, location, occur), left_join) %>%
    names() 

obistools::report %>%
    View()

```


