---
title: "Taxonomix Matchup to WoRMs"
author: "Sebastian DiGeronimo"
date: "3/2/2022"
output: html_document
---
# Load Packages
This loads any packages that are not contained in the .Rprofile.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
librarian::shelf(
    readxl, hablar, worrms, ggforce, geosphere, vroom, plotly,
    
    quiet = TRUE
    )

# set location for aphiaID list and the base of the file name
file_exprs <- file_expr(loc       = here::here("data", "metadata", "aphia_id"), 
                        file_base = "aphia_taxa")
```

# Load functions if not already
This will load the functions needed if not already contain in the search path.
```{r load-functions}
source(here("scripts", "attach_funcs.R"))
func_attach()
rm(func_attach)
```

# Load Data Sets
1. Parse all file paths in `data/raw/`. 
- Will ignore files with `~$` in front because these are usually temporary files that are opened
2. `skip_file` will check a processed file to ignore files that were previously loaded and formatted
3. Take leftover files and merge data into long format
- files contain some metadata for each sample processed and taxonomic data
```{r load-taxa-data}
# ---- find all file names and filter ----
file.taxa <- 
    fs::dir_ls(path    =  here::here("data", "raw"),
               regexp  = "^[^~]*\\.xlsx$",
               recurse = TRUE) %>%
    skip_file(., check = "ignore")

# ---- load all files into one tibble ----
taxa_data <-
    file.taxa %>%
    mutate(
    data = map(files, ~ load_data(.x, verbose = FALSE))
    ) %>%
    unnest(data)
    
head(taxa_data)
distinct(taxa_data, cruise_id, site, mesh)
```
# Merge taxanomic list and save new data
1. Loads previous master taxonomic data sheet
- This will check if it is contained in the cloud service at the root level, or 
  create one if it doesn't exists
- If it doesn't exist, it will check the long format data for a column called 
  taxa
- Then, it will `clean` the taxa list by removing sp., spp. and life stage 
  information
- Next, it will use the custom function for searching scientific names in `WoRMS`
    - The base for this is `obistools::match_taxa`, but it adds the ability to 
      type non-matched names or to use common names
- Once completed, it will save the taxa list in `data/metadata/aphia_id/` 
- It will check if there are any `NAs` if set `check = TRUE`
    - Only the `NA` rows will be rechecked again. Then it will merge with 
      previous sheet with a new filename 

```{r aphia-ids}
taxa <-
  taxa_data %>%
  select(taxa) %>%
  distinct() %>%
  merge_taxa(.,
    .file_expr = file_exprs,
    check      = FALSE,
    use_cloud  = TRUE,
    # viewer     = TRUE,
    # .recurse   = TRUE
  )

taxa

# Check unmatched taxa names
taxa %>%
    filter(is.na(scientificName))
```
# Push current master taxonomic sheet to cloud
1. Load most recent taxa sheet
2. Check if there any unmatched taxa
3. Push to cloud location

```{r push-pull-save}
# current saved
taxa_list <- 
  load_taxa_list(
    loc = file_exprs[[3]],
    check = TRUE)

# join to current taxa list to add new ones
taxa_list <- 
    taxa_list %>%
    full_join(taxa) 

# check if there are not matches and decide if want to fix
no_matches <- taxa_list %>%
    filter(is.na(scientificName))
no_matches

# uncomment if want to fix unmatched names
# taxa_list <-
#     taxa_unmatch(taxa,
#                  lifestage,
#                  .file_expr = file_exprs,
#                  save_file  = FALSE,
#                  viewer     = TRUE)

if (FALSE) {
  update_taxa_list(
      taxa_list   = taxa_list,
      .cloud_dir  = here(cloud_dir),
      save        = TRUE,
      # overwrite = FALSE,
      .file_expr  = file_exprs,
      where_to    = "local"
  )
    
    # master_taxa_list(taxa_list  = taxa_list, 
    #                  .cloud_dir = cloud_dir, 
    #                  where_to   = "local",
    #                  save       = TRUE)
}
```

2. Merge taxa lists with long format abundance data.
- Taxa list contains `taxa_orig`, `taxa`, `lifeStage`, `scientificName`, `scientificNameID`, `match_type`

3. Save merged data in merged long format and separate samples files in 
`data/processed/` and `data/processed/ind_file_merg/` 
- If supplied `file.taxa`, it will append `processed_files.csv` (the list of 
  files) to skip in future runs in `data/metadata/`
```{r merge-data-aphia-ids}
taxa_matched_merg <- 
    right_join(taxa, taxa_data, by = c("taxa_orig" = "taxa")) %>%
    relocate(files, cruise_id, date_collected, site, .before = 1) %>%
    arrange(date_collected, site, taxa)

# save data and append processed files log
if (FALSE) {
    save_merged(taxa_matched_merg = taxa_matched_merg, 
                .taxa_file        = file.taxa,
                loc               = here("data", "processed"),
                ind_file          = TRUE,
                append            = FALSE # if starting over, create new merged
                )
}
```


# Copy Processed Files to Cloud
```{r}
# ============================================================================ #
# ---- Check if Cloud Directory is Set and Exists ----
# ============================================================================ #    
if (is.na(cloud_dir) | !dir_exists(cloud_dir)) {
    rlang::abort("Stop! You don't have a cloud directory set!")
}

# ============================================================================ #
# ---- Update Master Taxonomic Sheet ----
# ============================================================================ #    
if (FALSE) {
     update_taxa_list(
      taxa_list   = taxa_list,
      .cloud_dir  = here(cloud_dir),
      save        = FALSE,
      # overwrite = FALSE,
      .file_expr  = file_exprs,
      where_to    = "cloud"
  )
}

# ============================================================================ #
# ---- Copy Individual Files ----
# ============================================================================ #    
if (FALSE) {
    copy_loc <- 
        here(cloud_dir, "processed", "ind_file_merg")
    
        dir_create(copy_loc)
        
        here("data", "processed", "ind_file_merg") %>%
        dir_ls() %>%
        tibble(files = .) %>%
        
       # extracting basename to use as name in cloud 
        mutate(base      = basename(files),
               base      = str_replace(base, "processed.*", "processed\\.csv"),
               new_files = here(copy_loc, base), 
               times     = select(file_info(files), birth_time)
               ) %>%
            
        # get most recent version to send to cloud
        unnest(times) %>%
        arrange(desc(birth_time)) %>%
        distinct(new_files, .keep_all = TRUE) %$%
    
        # copying individual files to cloud and overwriting previous one      
        file_copy(path = files, new_path = new_files, overwrite = TRUE)
}

# ============================================================================ #
# ---- Copy Recent Merged File ----
# ============================================================================ #    
if (FALSE) {
   copy_loc <- 
       here(cloud_dir, "processed")
       dir_create(copy_loc)
    
       here("data", "processed") %>%
       dir_ls(regexp = "all_merged") %>%
       last_mod() %>%
       tibble(files = .) %>%
         
       # extracting basename to use as name in cloud 
       mutate(base      = basename(files),
              base      = str_replace(base, "processed.*", "processed\\.csv"),
              new_files = here(copy_loc, base)) %$% 
       
       # copying merged file to cloud and overwriting previous one    
       file_copy(path      = files, 
                 new_path  = new_files, 
                 overwrite = TRUE)
}
```

