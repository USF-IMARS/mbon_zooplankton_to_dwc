---
title: "OBIS Event Creations"
author: "Sebastian DiGeronimo"
date: '2022-09-02'
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---
TODO: check how to save UTF-8 encoding on csv files 
TODO: station depth and uncertainty
TODO: 

# 1.0 Load
## 1.1 Libraries and Calculations

```{r setup}
if (!nzchar(system.file(package = "librarian"))) 
    install.packages("librarian")

librarian::shelf(
    quiet = TRUE,
    librarian, conflicted, ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr,
    forcats, lubridate, glue, fs, magrittr, here,
    
    # additional
    
)

conflicts_prefer(
  dplyr::filter(), 
  dplyr::select()
  )

```

## 1.2 Larval Stages: Accepted Version


```{r other-vars}
larval_stage <- 
    tribble(
        # original name  # DwC versions
        ~larva_orig,     ~larva_new,
       "copepodite", "copepodites",
          "nauplii",     "nauplii", 
           "larvae",      "larvae",
           "larva",      "larvae",
         "juvenile",    "juvenile",
             "eggs",         "egg",
              "egg",         "egg",
             "zoea",       "zoeae",
        "protozoea",  "protozoeae",
           "cypris",      "cypris",
         "megalopa",    "megalopae"
    )

```

## 1.3 Load functions, if not already
This will load the functions needed if not already contain in the search path.
```{r load-functions}
source(here("scripts", "attach_funcs.R"))
func_attach()
rm(func_attach)
```
## 1.4 Info on Workflow
Workflow:
<https://github.com/ioos/bio_data_guide/tree/main/OBIS_data_tiers>
3 "Cores": 
   1. Event - where/when took place
   2. Occurrence -  species info, presence/absence 
   3. Measurement or Fact (MoF) - if any, other necessary and environmental 
                                  quantities measured

Steps:
1. Read in previous data
2. Add additional taxonomic information using aphia ID
3. Merge metadata and calculate densities
4. Add columns to convert to DarwinCore
5. Extract each core
  - EventCore
  - OccurrenceCore
  - MoFcore


## 1.5 Load data for OBIS conversion
```{r load-data}
# ---- search file ---- #
dwc_info_path <-
  # here("data", "processed", "pre_obis") %>%
  here(cloud_dir, "processed") %>%
  dir_ls(regexp = "zoo_data_pre_obis") %>%
  last_mod() %T>%
  print()

# ---- load file ---- #
dwc_info_raw <-
  dwc_info_path %>%
  read_csv(
    show_col_types = FALSE,
    guess_max      = 4000
  ) %>%
  left_join(
    # other_vars$larval_stage,
    larval_stage,
    by = c("lifeStage" = "larva_orig")
  ) %>%
  print()
```

## 1.6 Load MoF Base Info
URL for controlled vocab: <http://vocab.nerc.ac.uk/collection/>

```{r mof-base}
# TODO: edit MoF for more information
# source mof file
source(here("scripts", "mof_base_create_table.R"))

mof_info <- list()

# ---- function to create/read base mof info
mof_info$mof_base <-
  mof_read() %T>%
  print() %>%
  filter(!is.na(measurementType))

mof_info$mof_base
```

# 2.0 DarwinCore/OBIS Conversion
## 2.1 Ex: Occurrence Bare Minimum example
set to `TRUE` if want to run
```{r obis-file}
# if (FALSE) {
#    data.table::data.table(
#           occurrenceID = c("my-dataset-0001c29"),
#        decimalLatitude = c(-87.7575),
#       decimalLongitude = c(24.4727),
#         scientificName = c("Sparisoma aurofrenatum"),
#       scientificNameID = c("urn:lsid:ipni.org:names:37829-1:1.3"),
#       occurrenceStatus = c("present"),
#          basisOfRecord = c("HumanObservation"),
#              datasetID = c("my-dataset-tylar-2020-01-08-123456"),
#              eventDate = c("2010-01-03T13:44Z")
#     )
# }
```

## 2.2 Add DarwinCore Columns
Here is where all the information to convert to DarwinCore format is created.
Later the columns will be extracted for each `core`. 

Spelling matters and can be check here: <https://dwc.tdwg.org/terms/>
```{r dwc-cols}
dwc_info_raw <-
  dwc_info_raw %>% 
  mutate(
    rowNumber = row_number(),
    .before   = 1
  ) %>%
 
# ============================================================================ #
# ---- 2.2.1 Record Level ---- #
# ============================================================================ #
  mutate(
    type                   = "Event",
    modified               = format(Sys.Date(), "%Y-%m-%d"),
    language               = "en",
    license                = "http://creativecommons.org/publicdomain/zero/1.0/legalcode",
    institutionCode        = "USF | NOAA", # include NOAA
    # ownerInstitutionCode = "USF",
    collectionCode         = "SEUS-MBON-Zooplankton",
    parentEventID          = "IMaRS_MBON_zooplankton",
    datasetName            = paste(
      "MBON/USF-IMaRS/NOAA AOML Florida Keys National",
      "Marine Sanctuary Zooplankton Net Tows"
    ),
    basisOfRecord          = "PreservedSpecimen", # "HumanObservation",
    recordedBy             = "NOAA AOML | USF IMaRS",
    identifiedBy           = "Natalia Lopez Figueroa",
    identifiedByID         = "https://orcid.org/0000-0002-7527-0481",
  ) %T>%
  print() %>%
    
# ============================================================================ #
# ---- 2.2.2 Event Level ---- #
# ============================================================================ #    
  mutate(
    # catalogNumber = # maybe add in the future
    locationID       = site,
    eventDate        = format_ISO8601(date_time, usetz = "Z"),
    eventID          = glue("{cruise_id}:stn{locationID}:{mesh}um:{date}"),
    # keeping same `eventID` from previous submission
    # cruise ID was updated afterwards
    eventID = str_replace(eventID,  "WS17282", "WS17275"),
    
    fieldNumber      = glue("{cruise_id}-{station}-{mesh}"),
    year             = year(date_time),
    month            = month(date_time),
    day              = day(date_time),
    samplingProtocol = glue(
      "{mesh} mesh size (um) - bongo nets",
      "folson splitter",
      "http://drs.nio.org/drs/handle/2264/95",
      .sep = " | "
    ),

    # TODO: habitat NERC vocab
    habitat          = case_match(
        locationID,
        c("MR", "LK", "WS", "9B") ~ "near coral reef",
        c("57", "54") ~ "near river mouth", 
        .default = NA),
    sampleSizeValue  = volume_filt_cubic_m,
    sampleSizeUnit   = "Volume per cubic metre of filtered seawater",
    samplingEffort   = str_c(tow_time_min, "minutes", sep = " "),
  ) %T>%
  print() %>%

# ============================================================================ #
# ---- 2.2.3 Location Level ---- #
# ============================================================================ #        
  mutate(
    decimalLatitude   = lat_in,
    decimalLongitude  = lon_in,
    higherGeographyID = case_match(
      site,
      c("MR", "LK", "WS", "9B") ~ "http://vocab.getty.edu/tgn/7030258",
      c("57", "54")             ~ "http://vocab.getty.edu/tgn/1101513",
      .default = "Not Found"
    ),
    higherGeography   = "North America | United States | Florida",
    continent         = "North America",
    country           = "United States",
    countryCode       = "US",
    stateProvince     = "Florida",
    geodeticDatum     = "EPSG:4326",
    georeferencedBy   = "NOAA AOML | USF IMaRS | RSMAS R/V Walton Smith",
    georeferenceVerificationStatus = "verified by contributor"
  ) %T>%
  print() %>%
    
# ============================================================================ #
# ---- 2.2.4 Occurrence Level ---- #
# ============================================================================ #    
  mutate(
    lifestage            = larva_new,
    larva_new            = if_else(is.na(larva_new), "unkwn", larva_new),
    # occurrenceID         = glue("{eventID}:{taxa_orig}:{lifeStage}"),
    occurrenceID         = glue("{eventID}:{taxa_orig}:{larva_new}"),
    occurrenceID         = str_replace_all(occurrenceID, "\\s", "_"),
    occurrenceID         = str_replace_all(occurrenceID, "\\.", ""),
    
    # keep `occurrenceID`, while lifeStage is larvae
    occurrenceID         = str_replace(occurrenceID, "Cyphonautes:larvae", "Cyphonautes:unkwn"),
    taxonRank            = rank,
    dateIdentified       = date_analyzed,
    # update June 26, 2023
    # change organismQuantity to a density of individuals per cubic meter
    # organismQuantity     = individualCount,
    # organismQuantityType = "Summation of 3x 5mL Aliquots",
    organismQuantity     = ind_m3,
    organismQuantityType = "individuals per cubic metre",
    occurrenceStatus     = "present",
    preparations         = str_c(
      "seawater and 10% formalin before analysis in cod ends",
      "70% ethanol after analysis in vials",
      sep = " | "
    ),
    identificationReferences = "WoRMS",
    verbatimIdentification   = taxa_orig,
    disposition = "in collection"
  ) %T>%
  print() %>%
    
# ============================================================================ #
# ---- 2.2.5 Measurement or Fact Additional Terms ---- #
# ============================================================================ #    
  mutate(
    net_type   = "bongo nets",
    microscopy = "microscopy"
  )
```

## 2.3 Load Pre-2018 Data 

### 2.3.1 Load Extended Aphia ID Information for Pre-2018 Data

```{r}
# aphia_id <-
#   # here("data", "metadata") %>%
#   here("data", "zoo_pre_2018") %>%
#   dir_ls(
#     # regexp = "aphia_extended_info",
#     # regexp = "taxa_worms_info",
#     regexp = "aphia_taxa_jaime",
#     recurse = 1
#   ) %>%
#   last_mod() %T>% 
#   print()
# 
# if (is_empty(aphia_id) | FALSE) {
#   # create extended aphia_id info if does not exist
#   cli_alert_info("Creating an extended aphia ID file.")
# 
#   # ---- load aphia ID ---- #
#   aphia_id <-
#     dir_ls(
#       # path = here("data", "metadata", "aphia_id"),
#       path = here("data", "zoo_pre_2018"),
#       regexp = "^[^~]*(aphia)+.*\\.csv$"
#     ) %>%
#     last_mod(.) %>%
#     read_csv(show_col_types = FALSE) %>%
#     
#     # extract extended info from WoRMS
#     mutate(
#       aphiaID = str_extract(scientificNameID, "\\d+"),
#       aphiaID = as.numeric(aphiaID),
#       info = pmap(
#         list(
#           aphiaID,
#           taxa_orig,
#           row_number()
#         ),
#         .f = worrms_info_extract # function in taxa_list_check.R
#       )
#     ) %>%
#     unnest(info, names_repair = janitor::make_clean_names) %>%
#     select(!contains("_2"), -c(2:6))
# 
#   cli_alert_info("\n\nSaving another `Aphia Extended` file!\n}")
#   save_csv(
#     .data         = aphia_id,
#     # save_location = here("data", "metadata", "aphia_extended"),
#     save_location = here("data", "zoo_pre_2018"),
#     save_name     = "taxa_worms_info",
#     overwrite     = TRUE,
#     verbose       = TRUE,
#     utf_8         = TRUE
#   )
# 
# } else {
#   aphia_id <-
#     aphia_id %>%
#     last_mod()
# 
#   cli_alert_info("Loading file: {.file {basename(aphia_id)}}")
# 
#   aphia_id <-
#     read_csv(
#       aphia_id,
#       show_col_types = FALSE
#     ) %T>% 
#     print()
# }
```

```{r}
# # TODO: add cloud dir
# old_data <- 
#     here("data", "zoo_pre_2018", "processed") %>%
#     dir_ls(regexp = "occurrences") %>%
#     last_mod() %>%
#     read_csv(show_col_types = FALSE) %T>% print() 
# 
# aphia_extended <-
#   old_data %>%
#   select(
#     verbatimIdentification, scientificNameID,
#   ) %>%
#   distinct() %>%
#   mutate(
#     aphia_id = if_else(!is.na(scientificNameID),
#       str_split(scientificNameID, ":",
#         simplify = TRUE
#       )[, 5],
#       NA_character_
#     ) %>%
#       as.numeric(.),
#     row = row_number(),
#     info = pmap(
#       list(
#         aphia_id,
#         # taxa_orig,
#         verbatimIdentification,
#         row
#       ),
#       # .progress = TRUE,
#       .f =
#         (\(.x, .y, .z)
#         tryCatch(
#           {
#             if (.z == 1) {
#               cat(sprintf(
#                 "\n%-4s | %-6s | names\n%s",
#                 "Row", "aphia",
#                 stringi::stri_dup("-", 37)
#               ))
#             }
#             cat(sprintf("\n%-4d | %6s | %s | ", .z, .x, .y))
#             x <- worrms::wm_record(.x)
#             cat(x$scientificname)
#             return(x)
#           },
#           error = function(e) {
#             return(NULL)
#           }
#         ))
#     )
#   ) %>%
#   unnest(info,
#     names_repair = janitor::make_clean_names,
#     keep_empty = TRUE
#   ) %>%
#   rename(
#     # might be need to go back to old way
#     # "scientificName"   = scientific_name
#     # "scientificNameID" = scientific_name_id
#     "scientificName"   = valid_name,
#     "scientificNameID" = lsid
#   ) %>%
#   mutate(
#     scientificNameID = str_replace(
#       scientificNameID, ":(\\d+)",
#       glue(":{valid_aphia_id}")
#     )
#   ) %T>% 
#   print()
# 
# old_data <- 
#   old_data %>%
#   select(-scientificName) %>%
#   left_join(.,
#     aphia_extended,
#     by = c(
#       "verbatimIdentification" = "verbatim_identification",
#       "scientificNameID"   = "scientific_name_id"
#     )
#   ) %T>% 
#   print()


```

```{r load-pre-2018}
old_data <-
  here("data", "zoo_pre_2018", "processed") %>%
  dir_ls(regexp = "occurrences") %>%
  last_mod() %>%
  read_csv(show_col_types = FALSE) %>%
  mutate(
    ind_m3 = if_else(is.infinite(ind_m3), NA, ind_m3),
    organismQuantity = if_else(is.infinite(organismQuantity), NA, organismQuantity),

    ) %>%
  rename(
    "disposition" = any_of("dispostion"),
    "number_ind_sample" = total_ind_correct
    ) %T>%
  print()

dwc_info <-
  old_data %>%
  select(any_of(names(dwc_info_raw))) %>%
  mutate(
    date_time    = ymd_hms(date_time),
    eventDate    = format_ISO8601(eventDate, usetz = "Z"),
    occurrenceID = stringi::stri_trans_general(occurrenceID, "Latin-ASCII"),
    occurrenceID = str_replace_all(occurrenceID, "\\s", "_"),
    occurrenceID = str_replace_all(occurrenceID, "\\.", ""),
    modified     = format(Sys.Date(), "%Y-%m-%d"),
  ) %>%
  # TODO: figure out what to do with these duplicates?
  filter(!str_detect(eventID, "2$")) %>%
  bind_rows(., dwc_info_raw) %>%
  mutate(
    modified = format(Sys.Date(), "%Y-%m-%d"),
    year,
    eventDate1 = as_date(eventDate),
    year       = if_else(
      is.na(year),
      year(eventDate1),
      year
    ),
    month = if_else(
      is.na(month),
      month(eventDate1),
      month
    ),
    day = if_else(
      is.na(day),
      day(eventDate1),
      day
    ),
  ) %>%
  mutate(
    across(
      contains("date") & where(is.Date) | where(is.POSIXct),
      \(x) format_ISO8601(x, usetz = "Z")
    )
  ) %T>%
  print()

dwc_info <- 
  dwc_info %>%
  mutate(
    eventID,
    sampleSizeValue,
    # replace org quantity when NA or <=0 with counts
    organismQuantity = if_else(
      is.na(volume_filt_cubic_m) | volume_filt_cubic_m <= 0, 
      number_ind_sample, 
      organismQuantity
    ),
    # replace org quantitytype when NA or <=0 with "count"
    organismQuantityType = if_else(
      is.na(volume_filt_cubic_m) | volume_filt_cubic_m <= 0, 
     "Count (in assayed sample) of biological entity specified elsewhere",
      organismQuantityType
    ),
    organismRemarks = if_else(
      is.na(volume_filt_cubic_m) | volume_filt_cubic_m <= 0, 
     "Counts of biological entity were used because volume filtered was not recorded, 0 m^3 or less than 0 m^3",
      NA
    ),
    # replace samplesizevalue when vol filt is NA or <= 0 to NA
    sampleSizeValue = if_else(
      is.na(volume_filt_cubic_m) | volume_filt_cubic_m <= 0, 
      NA, 
      sampleSizeValue
    ),
    eventRemarks = vol_notes
  ) 
```


```{r}
filter(old_data, 
       str_detect(eventID, "2$"))
old_data %>%
  # select(any_of(names(dwc_info_raw))) %>%
  mutate(
    date_time    = ymd_hms(date_time),
    eventDate    = format_ISO8601(eventDate, usetz = "Z"),
    occurrenceID2 = stringi::stri_trans_general(occurrenceID, "Latin-ASCII"),
    occurrenceID2        = str_replace_all(occurrenceID, "\\s", "_"),
    occurrenceID2        = str_replace_all(occurrenceID2, "\\.", ""),
    modified     = format(Sys.Date(), "%Y-%m-%d"),
  )  %>%
    select(occurrenceID, occurrenceID2) %>%
    filter(occurrenceID != occurrenceID2)
```



```{r}
save_csv(
  .data          = dwc_info,
  save_location  =  here("data", "processed", Sys.Date()),
  save_name      = "zooplankton_erddap",
  # overwrite      = TRUE,           # <-- uncomment to overwrite
  verbose        = TRUE,
  time_stamp_fmt = NULL,
  utf_8          = TRUE
  )

if (FALSE && exists("cloud_dir")) {
# if (TRUE && exists("cloud_dir")) {
  copy_file_name <-
    here("data", "processed") %>%
    dir_ls() %>%
    str_subset("zooplankton_erddap\\.csv")

  dir_create(here(cloud_dir, "processed"))

  file_copy(
    copy_file_name,
    here(cloud_dir, "processed", "obis"),
    FALSE
    # TRUE
  )
}

```



# 3.0 Extract "Cores"
The information from each core will be extracted into it's individual `core` to 
save. The actual column names need to match the ones from section `2.2`. This 
section allows the `user` to add/subtract columns from a specific core as 
needed.

## 3.1 EventCore
```{r event-extract}
event_core <-
  dwc_info %>%
  select(
    type,
    modified,
    license,
    institutionCode,
    datasetName,
    language,
    locationID,
    eventDate,
    eventID,
    fieldNumber,
    year,
    month,
    day,
    samplingProtocol,
    habitat,
    sampleSizeValue,
    sampleSizeUnit,
    samplingEffort,
    decimalLatitude,
    decimalLongitude,
    higherGeographyID,
    higherGeography,
    continent,
    country,
    countryCode,
    stateProvince,
    geodeticDatum,
    georeferencedBy,
    coordinateUncertaintyInMeters,
    minimumDepthInMeters,
    maximumDepthInMeters,
    eventRemarks
  ) %>%
  distinct() %>%
  fill(1:language, .direction = "up") %T>%
    
  # keeping same `eventID` from previous submission
  # cruise ID was updated afterwards
  # mutate(eventID = str_replace(eventID,  "WS17282", "WS17275")) %T>% 
  print()

naniar::vis_miss(event_core)
event_core %>%
  select(eventRemarks, sampleSizeValue:samplingEffort)
  filter(sampleSizeValue <= 0 | is.na(sampleSizeValue))

janitor::get_dupes(event_core, fieldNumber)
```

## 3.2 Occurrence Core
```{r occur-extract}
occur_core <-
  dwc_info %>%
  select(
    eventID,
    occurrenceID,
    collectionCode,
    scientificName,
    kingdom:genus, # TODO: check if species column exists
    taxonRank,
    # recorded by
    identifiedBy,
    identifiedByID,
    dateIdentified,
    organismQuantity,
    organismQuantityType,
    lifeStage,
    occurrenceStatus,
    preparations,
    scientificNameID,
    basisOfRecord,
    identificationReferences,
    verbatimIdentification,
    georeferenceVerificationStatus,
    disposition,
    organismRemarks
  )  %>%
  fill(collectionCode, .direction = "up") %>%
  filter(!is.na(scientificName)) %T>% 
    
  # # keeping same `eventID` from previous submission
  # # cruise ID was updated afterwards
  # mutate(eventID = str_replace(eventID,  "WS17282", "WS17275")) %T>%
  print()

naniar::vis_miss(occur_core)
occur_core %>%
  # select(dateIdentified:lifeStage) %>%
  # filter(str_detect(organismQuantityType, "Count"))
  filter(!is.na(organismRemarks))

```

## 3.3 Measurement or Fact Core (MoF)

### 3.3.1 Convert MoF

```{r mof-extract}
# ============================================================================ #
# ---- Extract and Convert MoF Event Info ----
# ============================================================================ #    
mof_info$mof_event <-
  dwc_info %>%
  
  # ---- filter for unique eventIDs
  distinct(eventID, .keep_all = TRUE) %>%
    
  # ---- select `event` mof variables    
  select(
    eventID,
    any_of(filter(mof_info$mof_base, str_detect(event_occur, "event"))$orig_term)
  ) %>%

  # ---- convert to long format based on eventID
  pivot_longer(
    cols      = -eventID,
    names_to  = "orig_term",
    values_to = "measurementValue",
    values_transform = list(measurementValue = as.character)
  )

# ============================================================================ #
# ---- Extract and Convert MoF Occurrence Info ----
# ============================================================================ #    
mof_info$mof_occur <-
  dwc_info %>%
  
  # ---- filter for unique eventIDs and occurrenceIDs
  distinct(eventID, occurrenceID, .keep_all = TRUE) %>%
    
  # ---- select `occurrence` mof variables     
  select(
    eventID, occurrenceID,
    any_of(filter(mof_info$mof_base, str_detect(event_occur, "occur"))$orig_term)
  ) %>%

  # ---- convert to long format based on eventID and occurrenceID
  pivot_longer(
    cols      = -c(eventID, occurrenceID),
    names_to  = "orig_term",
    values_to = "measurementValue",
    values_transform = list(measurementValue = as.character)
  ) 

# ============================================================================ #
# ---- Merge Event and Occurrence MoF with MoF Base ----
# ============================================================================ #    
mof_core <-
  bind_rows(
    mof_info$mof_occur,
    mof_info$mof_event
  ) %>%
  left_join(mof_info$mof_base, by = "orig_term") %>%
  arrange(eventID, event_occur) %>%
  select(
    -orig_term,
    -event_occur,
    -measurementAccuracy # maybe will update later
  ) %T>% 
  print()

mof_core <- 
  dwc_info %>%
  select(eventID, "measurementRemarks" = vol_notes) %>%
  mutate(
   measurementType = "Sample volume (filtration) by measuring cylinder"
  ) %>%
  left_join(
    mof_core, 
    .
  ) %>%
  distinct() %T>% 
  print()

slice_sample(mof_core, n = 1, by = eventID)
cli::cli_alert_info("Dimensions: {dim(mof_core)}")

naniar::vis_miss(mof_core)

mof_core %>%
  filter(is.na(measurementValue)) %>%
  arrange(measurementType)

mof_core %>%
  distinct(measurementType)
```


# 4.0 Examples to save

```{r dup-check}
janitor::get_dupes(event_core)
janitor::get_dupes(event_core, eventID)
janitor::get_dupes(occur_core)
janitor::get_dupes(occur_core, occurrenceID)
janitor::get_dupes(mof_core) # splits_analyzed and split_size are different, but no definition

occur_core
```
## 4.1 Show Missing Data
```{r}
naniar::vis_miss(event_core)
naniar::vis_miss(occur_core)
naniar::vis_miss(mof_core)
```

## 4.2 Save A Subset of Data, 5 samples
```{r save-subset}
set.seed(1234)
events <- sample(unique(event_core$eventID), 5)

pwalk(
  list(
    list(
      filter(event_core, eventID %in% events),
      filter(occur_core, eventID %in% events),
      filter(mof_core, eventID %in% events)
    ),
    here("data", "processed", "obis_example", Sys.Date()),
    glue("{c(\"event\", \"occur\", \"mof\")}_example_update")
  ),
  \(data, loc, name_f)
    save_csv(
      .data          = data,
      save_location  = loc,
      save_name      = name_f,
      # overwrite      = TRUE,          # <-- uncomment to overwrite
      verbose        = TRUE,
      time_stamp_fmt = "%Y-%m-%d",
      utf_8          = TRUE
    )
)
```

## 4.3 Save All Data
```{r save-full}
pwalk(
  list(
    list(
      event_core,
      occur_core,
      # mof_core
      filter(mof_core, !is.na(measurementValue))
    ),
    here("data", "processed", "obis", Sys.Date()),
    glue("{c(\"event\", \"occur\", \"mof\")}")
  ),
  \(data, loc, name_f)
    save_csv(
      .data          = data,
      save_location  = loc,
      save_name      = name_f,
      # overwrite      = TRUE,        # <-- uncomment to overwrite
      verbose        = TRUE,
      time_stamp_fmt = NULL,
      utf_8 = TRUE
    )
)

if (FALSE && exists("cloud_dir")) {
# if (TRUE && exists("cloud_dir")) {
  obis_files <-
    here("data", "processed", "obis") %>%
    dir_ls(type = "file", recurse = 1) %>%
    str_subset("old|ex", negate = TRUE) %>%
    str_subset("2025-09-24") %T>% 
    print()
  stop("check date to be sure using the correct file")
  dir_create(here(cloud_dir, "processed", "obis"))

  file_copy(
    obis_files,
    here(cloud_dir, "processed", "obis"),
    overwrite = TRUE                 # <-- uncomment to overwrite
  )
}
```


# ---- TEST ----
```{r test}
str_count(occur_core$occurrenceID, "\\.") %>%
    sort(decreasing = TRUE)
occur_core %>%
    filter(str_detect(occurrenceID, "\\.")) %>%
    select(occurrenceID) %>%
    mutate(
        mult = str_count(occurrenceID, "\\."),
        occ2 = str_replace_all(occurrenceID, "\\.", "")
    ) %>%
      filter(mult > 1)

reduce(list(event_core, occur_core, mof_core), left_join) 

left_join(dwc_info,
        mof_core,
        by = c(
            "eventID",
            "occurrenceID"
        )
        ) %>%
          naniar::vis_miss()
    
obistools::report() %>%
    View()

```

```{r}
obistools::check_extension_eventids(event_core, mof_core, field = "eventID")


str_subset(obistools::occurrence_fields(), "verbatim")
str_subset(obistools::occurrence_fields(), "(?i)remark")
str_subset(obistools::event_fields(), "(?i)remark")

str_sort(obistools::occurrence_fields())
str_sort(obistools::event_fields())
obistools::event_fields()
obistools::abra %>%
    obistools::report_summary(50)

select(dwc_info, any_of(obistools::occurrence_fields()))

names(rename(occur_core, "disposition" = dispostion)) %in% obistools::occurrence_fields()
# verbatimIdentification


str_subset(names(occur_core), str_c(obistools::occurrence_fields(), collapse = "|"),
           negate = TRUE)
str_subset(names(event_core), str_c(obistools::event_fields(), collapse = "|"),
           negate = TRUE)
```

```{r}
set.seed(1234)
dwc_info %>%
    filter(!is.na(recordedBy)) %>%
    nest(.by = cruise_id) %>%
    slice_sample(n = 2) %>%
    unnest(data) %>%
    slice_head(by = cruise_id, prop = 0.2) %>%
    writexl::write_xlsx(
      here("data", "processed", "obis", "ioos_subsample.xlsx")
        )

shell.exec(here("data", "processed", "obis", "ioos_subsample.xlsx"))
shell.exec(here("data", "processed", "obis"))
```

